From c028af111ad3de67fed8b2c525bbd2cba94cfcbc Mon Sep 17 00:00:00 2001
From: Matthew Poremba <matthew.poremba@amd.com>
Date: Wed, 8 Dec 2021 12:23:23 -0600
Subject: [PATCH 009/757] arch-gcn3,gpu-compute: Move TLB to common folder in
 amdgpu

This TLB is more of an "APU" TLB than anything GCN3 specific. It can be
used with either GCN3 or Vega. With this change, VEGA_X86 builds and one
can run binaries with Vega ISA code using the same steps as GCN3 but
building the Vega ISA instead.

Change-Id: I0c92bcd0379a18628dc05cb5af070bdc7e692c7c
Reviewed-on: https://gem5-review.googlesource.com/c/public/gem5/+/53803
Maintainer: Bobby Bruce <bbruce@ucdavis.edu>
Reviewed-by: Matt Sinclair <mattdsinclair@gmail.com>
Maintainer: Matt Sinclair <mattdsinclair@gmail.com>
Tested-by: kokoro <noreply+kokoro@google.com>
---
 src/arch/amdgpu/common/SConscript       |   43 +
 src/arch/amdgpu/common/X86GPUTLB.py     |   76 ++
 src/arch/amdgpu/common/tlb.cc           | 1460 +++++++++++++++++++++++++++++++
 src/arch/amdgpu/common/tlb.hh           |  445 ++++++++++
 src/arch/amdgpu/common/tlb_coalescer.cc |  539 ++++++++++++
 src/arch/amdgpu/common/tlb_coalescer.hh |  226 +++++
 src/arch/amdgpu/gcn3/SConscript         |    4 -
 src/arch/amdgpu/gcn3/X86GPUTLB.py       |   76 --
 src/arch/amdgpu/gcn3/gpu_isa.hh         |    2 +-
 src/arch/amdgpu/gcn3/tlb.cc             | 1460 -------------------------------
 src/arch/amdgpu/gcn3/tlb.hh             |  445 ----------
 src/arch/amdgpu/gcn3/tlb_coalescer.cc   |  539 ------------
 src/arch/amdgpu/gcn3/tlb_coalescer.hh   |  226 -----
 src/gpu-compute/compute_unit.cc         |    1 +
 src/gpu-compute/fetch_unit.cc           |    1 +
 src/gpu-compute/shader.cc               |    1 +
 16 files changed, 2793 insertions(+), 2751 deletions(-)
 create mode 100644 src/arch/amdgpu/common/SConscript
 create mode 100644 src/arch/amdgpu/common/X86GPUTLB.py
 create mode 100644 src/arch/amdgpu/common/tlb.cc
 create mode 100644 src/arch/amdgpu/common/tlb.hh
 create mode 100644 src/arch/amdgpu/common/tlb_coalescer.cc
 create mode 100644 src/arch/amdgpu/common/tlb_coalescer.hh
 delete mode 100644 src/arch/amdgpu/gcn3/X86GPUTLB.py
 delete mode 100644 src/arch/amdgpu/gcn3/tlb.cc
 delete mode 100644 src/arch/amdgpu/gcn3/tlb.hh
 delete mode 100644 src/arch/amdgpu/gcn3/tlb_coalescer.cc
 delete mode 100644 src/arch/amdgpu/gcn3/tlb_coalescer.hh

diff --git a/src/arch/amdgpu/common/SConscript b/src/arch/amdgpu/common/SConscript
new file mode 100644
index 0000000..f6f9cb6
--- /dev/null
+++ b/src/arch/amdgpu/common/SConscript
@@ -0,0 +1,43 @@
+# Copyright (c) 2021 Advanced Micro Devices, Inc.
+# All rights reserved.
+#
+# For use for simulation and test purposes only
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+#
+# 1. Redistributions of source code must retain the above copyright notice,
+# this list of conditions and the following disclaimer.
+#
+# 2. Redistributions in binary form must reproduce the above copyright notice,
+# this list of conditions and the following disclaimer in the documentation
+# and/or other materials provided with the distribution.
+#
+# 3. Neither the name of the copyright holder nor the names of its
+# contributors may be used to endorse or promote products derived from this
+# software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+import sys
+
+Import('*')
+
+if not env['BUILD_GPU']:
+    Return()
+
+if env['TARGET_GPU_ISA'] == 'gcn3' or env['TARGET_GPU_ISA'] == 'vega':
+    SimObject('X86GPUTLB.py', sim_objects=['X86GPUTLB', 'TLBCoalescer'])
+
+    Source('tlb.cc')
+    Source('tlb_coalescer.cc')
diff --git a/src/arch/amdgpu/common/X86GPUTLB.py b/src/arch/amdgpu/common/X86GPUTLB.py
new file mode 100644
index 0000000..b5f3387
--- /dev/null
+++ b/src/arch/amdgpu/common/X86GPUTLB.py
@@ -0,0 +1,76 @@
+# Copyright (c) 2011-2015 Advanced Micro Devices, Inc.
+# All rights reserved.
+#
+# For use for simulation and test purposes only
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+#
+# 1. Redistributions of source code must retain the above copyright notice,
+# this list of conditions and the following disclaimer.
+#
+# 2. Redistributions in binary form must reproduce the above copyright notice,
+# this list of conditions and the following disclaimer in the documentation
+# and/or other materials provided with the distribution.
+#
+# 3. Neither the name of the copyright holder nor the names of its
+# contributors may be used to endorse or promote products derived from this
+# software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from m5.defines import buildEnv
+from m5.params import *
+from m5.proxy import *
+
+from m5.objects.ClockedObject import ClockedObject
+from m5.SimObject import SimObject
+
+class X86GPUTLB(ClockedObject):
+    type = 'X86GPUTLB'
+    cxx_class = 'gem5::X86ISA::GpuTLB'
+    cxx_header = 'arch/amdgpu/common/tlb.hh'
+    size = Param.Int(64, "TLB size (number of entries)")
+    assoc = Param.Int(64, "TLB associativity")
+
+    if buildEnv.get('FULL_SYSTEM', False):
+        walker = Param.X86PagetableWalker(X86PagetableWalker(),
+                                          "page table walker")
+
+    hitLatency = Param.Int(2, "Latency of a TLB hit")
+    missLatency1 = Param.Int(5, "Latency #1 of a TLB miss")
+    missLatency2 = Param.Int(100, "Latency #2 of a TLB miss")
+    maxOutstandingReqs = Param.Int(64, "# of maximum outstanding requests")
+    cpu_side_ports = VectorResponsePort("Ports on side closer to CPU/CU")
+    slave    = DeprecatedParam(cpu_side_ports,
+                        '`slave` is now called `cpu_side_ports`')
+    mem_side_ports = VectorRequestPort("Ports on side closer to memory")
+    master   = DeprecatedParam(mem_side_ports,
+                        '`master` is now called `mem_side_ports`')
+    allocationPolicy = Param.Bool(True, "Allocate on an access")
+    accessDistance = Param.Bool(False, "print accessDistance stats")
+
+class TLBCoalescer(ClockedObject):
+    type = 'TLBCoalescer'
+    cxx_class = 'gem5::TLBCoalescer'
+    cxx_header = 'arch/amdgpu/common/tlb_coalescer.hh'
+
+    probesPerCycle = Param.Int(2, "Number of TLB probes per cycle")
+    coalescingWindow = Param.Int(1, "Permit coalescing across that many ticks")
+    cpu_side_ports = VectorResponsePort("Port on side closer to CPU/CU")
+    slave    = DeprecatedParam(cpu_side_ports,
+                        '`slave` is now called `cpu_side_ports`')
+    mem_side_ports = VectorRequestPort("Port on side closer to memory")
+    master   = DeprecatedParam(mem_side_ports,
+                        '`master` is now called `mem_side_ports`')
+    disableCoalescing = Param.Bool(False,"Dispable Coalescing")
diff --git a/src/arch/amdgpu/common/tlb.cc b/src/arch/amdgpu/common/tlb.cc
new file mode 100644
index 0000000..e108a0b
--- /dev/null
+++ b/src/arch/amdgpu/common/tlb.cc
@@ -0,0 +1,1460 @@
+/*
+ * Copyright (c) 2011-2021 Advanced Micro Devices, Inc.
+ * All rights reserved.
+ *
+ * For use for simulation and test purposes only
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * 3. Neither the name of the copyright holder nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ *
+ */
+
+#include "arch/amdgpu/common/tlb.hh"
+
+#include <cmath>
+#include <cstring>
+
+#include "arch/x86/faults.hh"
+#include "arch/x86/insts/microldstop.hh"
+#include "arch/x86/page_size.hh"
+#include "arch/x86/pagetable.hh"
+#include "arch/x86/pagetable_walker.hh"
+#include "arch/x86/regs/misc.hh"
+#include "arch/x86/regs/msr.hh"
+#include "arch/x86/x86_traits.hh"
+#include "base/bitfield.hh"
+#include "base/logging.hh"
+#include "base/output.hh"
+#include "base/trace.hh"
+#include "cpu/base.hh"
+#include "cpu/thread_context.hh"
+#include "debug/GPUPrefetch.hh"
+#include "debug/GPUTLB.hh"
+#include "mem/packet_access.hh"
+#include "mem/page_table.hh"
+#include "mem/request.hh"
+#include "sim/process.hh"
+#include "sim/pseudo_inst.hh"
+
+namespace gem5
+{
+namespace X86ISA
+{
+
+    GpuTLB::GpuTLB(const Params &p)
+        : ClockedObject(p), configAddress(0), size(p.size),
+          cleanupEvent([this]{ cleanup(); }, name(), false,
+                       Event::Maximum_Pri),
+          exitEvent([this]{ exitCallback(); }, name()), stats(this)
+    {
+        assoc = p.assoc;
+        assert(assoc <= size);
+        numSets = size/assoc;
+        allocationPolicy = p.allocationPolicy;
+        hasMemSidePort = false;
+        accessDistance = p.accessDistance;
+
+        tlb.assign(size, TlbEntry());
+
+        freeList.resize(numSets);
+        entryList.resize(numSets);
+
+        for (int set = 0; set < numSets; ++set) {
+            for (int way = 0; way < assoc; ++way) {
+                int x = set * assoc + way;
+                freeList[set].push_back(&tlb.at(x));
+            }
+        }
+
+        FA = (size == assoc);
+
+        /**
+         * @warning: the set-associative version assumes you have a
+         * fixed page size of 4KB.
+         * If the page size is greather than 4KB (as defined in the
+         * X86ISA::PageBytes), then there are various issues w/ the current
+         * implementation (you'd have the same 8KB page being replicated in
+         * different sets etc)
+         */
+        setMask = numSets - 1;
+
+        maxCoalescedReqs = p.maxOutstandingReqs;
+
+        // Do not allow maxCoalescedReqs to be more than the TLB associativity
+        if (maxCoalescedReqs > assoc) {
+            maxCoalescedReqs = assoc;
+            cprintf("Forcing maxCoalescedReqs to %d (TLB assoc.) \n", assoc);
+        }
+
+        outstandingReqs = 0;
+        hitLatency = p.hitLatency;
+        missLatency1 = p.missLatency1;
+        missLatency2 = p.missLatency2;
+
+        // create the response ports based on the number of connected ports
+        for (size_t i = 0; i < p.port_cpu_side_ports_connection_count; ++i) {
+            cpuSidePort.push_back(new CpuSidePort(csprintf("%s-port%d",
+                                  name(), i), this, i));
+        }
+
+        // create the request ports based on the number of connected ports
+        for (size_t i = 0; i < p.port_mem_side_ports_connection_count; ++i) {
+            memSidePort.push_back(new MemSidePort(csprintf("%s-port%d",
+                                  name(), i), this, i));
+        }
+    }
+
+    // fixme: this is never called?
+    GpuTLB::~GpuTLB()
+    {
+        // make sure all the hash-maps are empty
+        assert(translationReturnEvent.empty());
+    }
+
+    Port &
+    GpuTLB::getPort(const std::string &if_name, PortID idx)
+    {
+        if (if_name == "cpu_side_ports") {
+            if (idx >= static_cast<PortID>(cpuSidePort.size())) {
+                panic("TLBCoalescer::getPort: unknown index %d\n", idx);
+            }
+
+            return *cpuSidePort[idx];
+        } else if (if_name == "mem_side_ports") {
+            if (idx >= static_cast<PortID>(memSidePort.size())) {
+                panic("TLBCoalescer::getPort: unknown index %d\n", idx);
+            }
+
+            hasMemSidePort = true;
+
+            return *memSidePort[idx];
+        } else {
+            panic("TLBCoalescer::getPort: unknown port %s\n", if_name);
+        }
+    }
+
+    TlbEntry*
+    GpuTLB::insert(Addr vpn, TlbEntry &entry)
+    {
+        TlbEntry *newEntry = nullptr;
+
+        /**
+         * vpn holds the virtual page address
+         * The least significant bits are simply masked
+         */
+        int set = (vpn >> PageShift) & setMask;
+
+        if (!freeList[set].empty()) {
+            newEntry = freeList[set].front();
+            freeList[set].pop_front();
+        } else {
+            newEntry = entryList[set].back();
+            entryList[set].pop_back();
+        }
+
+        *newEntry = entry;
+        newEntry->vaddr = vpn;
+        entryList[set].push_front(newEntry);
+
+        return newEntry;
+    }
+
+    GpuTLB::EntryList::iterator
+    GpuTLB::lookupIt(Addr va, bool update_lru)
+    {
+        int set = (va >> PageShift) & setMask;
+
+        if (FA) {
+            assert(!set);
+        }
+
+        auto entry = entryList[set].begin();
+        for (; entry != entryList[set].end(); ++entry) {
+            int page_size = (*entry)->size();
+
+            if ((*entry)->vaddr <= va && (*entry)->vaddr + page_size > va) {
+                DPRINTF(GPUTLB, "Matched vaddr %#x to entry starting at %#x "
+                        "with size %#x.\n", va, (*entry)->vaddr, page_size);
+
+                if (update_lru) {
+                    entryList[set].push_front(*entry);
+                    entryList[set].erase(entry);
+                    entry = entryList[set].begin();
+                }
+
+                break;
+            }
+        }
+
+        return entry;
+    }
+
+    TlbEntry*
+    GpuTLB::lookup(Addr va, bool update_lru)
+    {
+        int set = (va >> PageShift) & setMask;
+
+        auto entry = lookupIt(va, update_lru);
+
+        if (entry == entryList[set].end())
+            return nullptr;
+        else
+            return *entry;
+    }
+
+    void
+    GpuTLB::invalidateAll()
+    {
+        DPRINTF(GPUTLB, "Invalidating all entries.\n");
+
+        for (int i = 0; i < numSets; ++i) {
+            while (!entryList[i].empty()) {
+                TlbEntry *entry = entryList[i].front();
+                entryList[i].pop_front();
+                freeList[i].push_back(entry);
+            }
+        }
+    }
+
+    void
+    GpuTLB::setConfigAddress(uint32_t addr)
+    {
+        configAddress = addr;
+    }
+
+    void
+    GpuTLB::invalidateNonGlobal()
+    {
+        DPRINTF(GPUTLB, "Invalidating all non global entries.\n");
+
+        for (int i = 0; i < numSets; ++i) {
+            for (auto entryIt = entryList[i].begin();
+                 entryIt != entryList[i].end();) {
+                if (!(*entryIt)->global) {
+                    freeList[i].push_back(*entryIt);
+                    entryList[i].erase(entryIt++);
+                } else {
+                    ++entryIt;
+                }
+            }
+        }
+    }
+
+    void
+    GpuTLB::demapPage(Addr va, uint64_t asn)
+    {
+
+        int set = (va >> PageShift) & setMask;
+        auto entry = lookupIt(va, false);
+
+        if (entry != entryList[set].end()) {
+            freeList[set].push_back(*entry);
+            entryList[set].erase(entry);
+        }
+    }
+
+
+
+    namespace
+    {
+
+    Cycles
+    localMiscRegAccess(bool read, MiscRegIndex regNum,
+                       ThreadContext *tc, PacketPtr pkt)
+    {
+        if (read) {
+            RegVal data = htole(tc->readMiscReg(regNum));
+            // Make sure we don't trot off the end of data.
+            pkt->setData((uint8_t *)&data);
+        } else {
+            RegVal data = htole(tc->readMiscRegNoEffect(regNum));
+            tc->setMiscReg(regNum, letoh(data));
+        }
+        return Cycles(1);
+    }
+
+    } // anonymous namespace
+
+    Fault
+    GpuTLB::translateInt(bool read, const RequestPtr &req, ThreadContext *tc)
+    {
+        DPRINTF(GPUTLB, "Addresses references internal memory.\n");
+        Addr vaddr = req->getVaddr();
+        Addr prefix = (vaddr >> 3) & IntAddrPrefixMask;
+
+        if (prefix == IntAddrPrefixCPUID) {
+            panic("CPUID memory space not yet implemented!\n");
+        } else if (prefix == IntAddrPrefixMSR) {
+            vaddr = (vaddr >> 3) & ~IntAddrPrefixMask;
+
+            MiscRegIndex regNum;
+            if (!msrAddrToIndex(regNum, vaddr))
+                return std::make_shared<GeneralProtection>(0);
+
+            req->setLocalAccessor(
+                [read,regNum](ThreadContext *tc, PacketPtr pkt)
+                {
+                    return localMiscRegAccess(read, regNum, tc, pkt);
+                }
+            );
+
+            return NoFault;
+        } else if (prefix == IntAddrPrefixIO) {
+            // TODO If CPL > IOPL or in virtual mode, check the I/O permission
+            // bitmap in the TSS.
+
+            Addr IOPort = vaddr & ~IntAddrPrefixMask;
+            // Make sure the address fits in the expected 16 bit IO address
+            // space.
+            assert(!(IOPort & ~0xFFFF));
+            if (IOPort == 0xCF8 && req->getSize() == 4) {
+                req->setLocalAccessor(
+                    [read](ThreadContext *tc, PacketPtr pkt)
+                    {
+                        return localMiscRegAccess(
+                                read, MISCREG_PCI_CONFIG_ADDRESS, tc, pkt);
+                    }
+                );
+            } else if ((IOPort & ~mask(2)) == 0xCFC) {
+                req->setFlags(Request::UNCACHEABLE | Request::STRICT_ORDER);
+                Addr configAddress =
+                    tc->readMiscRegNoEffect(MISCREG_PCI_CONFIG_ADDRESS);
+                if (bits(configAddress, 31, 31)) {
+                    req->setPaddr(PhysAddrPrefixPciConfig |
+                            mbits(configAddress, 30, 2) |
+                            (IOPort & mask(2)));
+                } else {
+                    req->setPaddr(PhysAddrPrefixIO | IOPort);
+                }
+            } else {
+                req->setFlags(Request::UNCACHEABLE | Request::STRICT_ORDER);
+                req->setPaddr(PhysAddrPrefixIO | IOPort);
+            }
+            return NoFault;
+        } else {
+            panic("Access to unrecognized internal address space %#x.\n",
+                  prefix);
+        }
+    }
+
+    /**
+     * TLB_lookup will only perform a TLB lookup returning true on a TLB hit
+     * and false on a TLB miss.
+     * Many of the checks about different modes have been converted to
+     * assertions, since these parts of the code are not really used.
+     * On a hit it will update the LRU stack.
+     */
+    bool
+    GpuTLB::tlbLookup(const RequestPtr &req,
+                      ThreadContext *tc, bool update_stats)
+    {
+        bool tlb_hit = false;
+    #ifndef NDEBUG
+        uint32_t flags = req->getFlags();
+        int seg = flags & SegmentFlagMask;
+    #endif
+
+        assert(seg != SEGMENT_REG_MS);
+        Addr vaddr = req->getVaddr();
+        DPRINTF(GPUTLB, "TLB Lookup for vaddr %#x.\n", vaddr);
+        HandyM5Reg m5Reg = tc->readMiscRegNoEffect(MISCREG_M5_REG);
+
+        if (m5Reg.prot) {
+            DPRINTF(GPUTLB, "In protected mode.\n");
+            // make sure we are in 64-bit mode
+            assert(m5Reg.mode == LongMode);
+
+            // If paging is enabled, do the translation.
+            if (m5Reg.paging) {
+                DPRINTF(GPUTLB, "Paging enabled.\n");
+                //update LRU stack on a hit
+                TlbEntry *entry = lookup(vaddr, true);
+
+                if (entry)
+                    tlb_hit = true;
+
+                if (!update_stats) {
+                    // functional tlb access for memory initialization
+                    // i.e., memory seeding or instr. seeding -> don't update
+                    // TLB and stats
+                    return tlb_hit;
+                }
+
+                stats.localNumTLBAccesses++;
+
+                if (!entry) {
+                    stats.localNumTLBMisses++;
+                } else {
+                    stats.localNumTLBHits++;
+                }
+            }
+        }
+
+        return tlb_hit;
+    }
+
+    Fault
+    GpuTLB::translate(const RequestPtr &req, ThreadContext *tc,
+                      Translation *translation, Mode mode,
+                      bool &delayedResponse, bool timing, int &latency)
+    {
+        uint32_t flags = req->getFlags();
+        int seg = flags & SegmentFlagMask;
+        bool storeCheck = flags & Request::READ_MODIFY_WRITE;
+
+        // If this is true, we're dealing with a request
+        // to a non-memory address space.
+        if (seg == SEGMENT_REG_MS) {
+            return translateInt(mode == Mode::Read, req, tc);
+        }
+
+        delayedResponse = false;
+        Addr vaddr = req->getVaddr();
+        DPRINTF(GPUTLB, "Translating vaddr %#x.\n", vaddr);
+
+        HandyM5Reg m5Reg = tc->readMiscRegNoEffect(MISCREG_M5_REG);
+
+        // If protected mode has been enabled...
+        if (m5Reg.prot) {
+            DPRINTF(GPUTLB, "In protected mode.\n");
+            // If we're not in 64-bit mode, do protection/limit checks
+            if (m5Reg.mode != LongMode) {
+                DPRINTF(GPUTLB, "Not in long mode. Checking segment "
+                        "protection.\n");
+
+                // Check for a null segment selector.
+                if (!(seg == SEGMENT_REG_TSG || seg == SYS_SEGMENT_REG_IDTR ||
+                    seg == SEGMENT_REG_HS || seg == SEGMENT_REG_LS)
+                    && !tc->readMiscRegNoEffect(MISCREG_SEG_SEL(seg))) {
+                    return std::make_shared<GeneralProtection>(0);
+                }
+
+                bool expandDown = false;
+                SegAttr attr = tc->readMiscRegNoEffect(MISCREG_SEG_ATTR(seg));
+
+                if (seg >= SEGMENT_REG_ES && seg <= SEGMENT_REG_HS) {
+                    if (!attr.writable && (mode == BaseMMU::Write ||
+                        storeCheck))
+                        return std::make_shared<GeneralProtection>(0);
+
+                    if (!attr.readable && mode == BaseMMU::Read)
+                        return std::make_shared<GeneralProtection>(0);
+
+                    expandDown = attr.expandDown;
+
+                }
+
+                Addr base = tc->readMiscRegNoEffect(MISCREG_SEG_BASE(seg));
+                Addr limit = tc->readMiscRegNoEffect(MISCREG_SEG_LIMIT(seg));
+                // This assumes we're not in 64 bit mode. If we were, the
+                // default address size is 64 bits, overridable to 32.
+                int size = 32;
+                bool sizeOverride = (flags & (AddrSizeFlagBit << FlagShift));
+                SegAttr csAttr = tc->readMiscRegNoEffect(MISCREG_CS_ATTR);
+
+                if ((csAttr.defaultSize && sizeOverride) ||
+                    (!csAttr.defaultSize && !sizeOverride)) {
+                    size = 16;
+                }
+
+                Addr offset = bits(vaddr - base, size - 1, 0);
+                Addr endOffset = offset + req->getSize() - 1;
+
+                if (expandDown) {
+                    DPRINTF(GPUTLB, "Checking an expand down segment.\n");
+                    warn_once("Expand down segments are untested.\n");
+
+                    if (offset <= limit || endOffset <= limit)
+                        return std::make_shared<GeneralProtection>(0);
+                } else {
+                    if (offset > limit || endOffset > limit)
+                        return std::make_shared<GeneralProtection>(0);
+                }
+            }
+
+            // If paging is enabled, do the translation.
+            if (m5Reg.paging) {
+                DPRINTF(GPUTLB, "Paging enabled.\n");
+                // The vaddr already has the segment base applied.
+                TlbEntry *entry = lookup(vaddr);
+                stats.localNumTLBAccesses++;
+
+                if (!entry) {
+                    stats.localNumTLBMisses++;
+                    if (timing) {
+                        latency = missLatency1;
+                    }
+
+                    if (FullSystem) {
+                        fatal("GpuTLB doesn't support full-system mode\n");
+                    } else {
+                        DPRINTF(GPUTLB, "Handling a TLB miss for address %#x "
+                                "at pc %#x.\n", vaddr,
+                                tc->pcState().instAddr());
+
+                        Process *p = tc->getProcessPtr();
+                        const EmulationPageTable::Entry *pte =
+                            p->pTable->lookup(vaddr);
+
+                        if (!pte && mode != BaseMMU::Execute) {
+                            // penalize a "page fault" more
+                            if (timing)
+                                latency += missLatency2;
+
+                            if (p->fixupFault(vaddr))
+                                pte = p->pTable->lookup(vaddr);
+                        }
+
+                        if (!pte) {
+                            return std::make_shared<PageFault>(vaddr, true,
+                                                               mode, true,
+                                                               false);
+                        } else {
+                            Addr alignedVaddr = p->pTable->pageAlign(vaddr);
+
+                            DPRINTF(GPUTLB, "Mapping %#x to %#x\n",
+                                    alignedVaddr, pte->paddr);
+
+                            TlbEntry gpuEntry(p->pid(), alignedVaddr,
+                                              pte->paddr, false, false);
+                            entry = insert(alignedVaddr, gpuEntry);
+                        }
+
+                        DPRINTF(GPUTLB, "Miss was serviced.\n");
+                    }
+                } else {
+                    stats.localNumTLBHits++;
+
+                    if (timing) {
+                        latency = hitLatency;
+                    }
+                }
+
+                // Do paging protection checks.
+                bool inUser = (m5Reg.cpl == 3 &&
+                               !(flags & (CPL0FlagBit << FlagShift)));
+
+                CR0 cr0 = tc->readMiscRegNoEffect(MISCREG_CR0);
+                bool badWrite = (!entry->writable && (inUser || cr0.wp));
+
+                if ((inUser && !entry->user) || (mode == BaseMMU::Write &&
+                     badWrite)) {
+                    // The page must have been present to get into the TLB in
+                    // the first place. We'll assume the reserved bits are
+                    // fine even though we're not checking them.
+                    return std::make_shared<PageFault>(vaddr, true, mode,
+                                                       inUser, false);
+                }
+
+                if (storeCheck && badWrite) {
+                    // This would fault if this were a write, so return a page
+                    // fault that reflects that happening.
+                    return std::make_shared<PageFault>(vaddr, true,
+                                                       BaseMMU::Write,
+                                                       inUser, false);
+                }
+
+
+                DPRINTF(GPUTLB, "Entry found with paddr %#x, doing protection "
+                        "checks.\n", entry->paddr);
+
+                int page_size = entry->size();
+                Addr paddr = entry->paddr | (vaddr & (page_size - 1));
+                DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, paddr);
+                req->setPaddr(paddr);
+
+                if (entry->uncacheable)
+                    req->setFlags(Request::UNCACHEABLE);
+            } else {
+                //Use the address which already has segmentation applied.
+                DPRINTF(GPUTLB, "Paging disabled.\n");
+                DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, vaddr);
+                req->setPaddr(vaddr);
+            }
+        } else {
+            // Real mode
+            DPRINTF(GPUTLB, "In real mode.\n");
+            DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, vaddr);
+            req->setPaddr(vaddr);
+        }
+
+        // Check for an access to the local APIC
+        if (FullSystem) {
+            LocalApicBase localApicBase =
+                tc->readMiscRegNoEffect(MISCREG_APIC_BASE);
+
+            Addr baseAddr = localApicBase.base * PageBytes;
+            Addr paddr = req->getPaddr();
+
+            if (baseAddr <= paddr && baseAddr + PageBytes > paddr) {
+                // Force the access to be uncacheable.
+                req->setFlags(Request::UNCACHEABLE);
+                req->setPaddr(x86LocalAPICAddress(tc->contextId(),
+                                                  paddr - baseAddr));
+            }
+        }
+
+        return NoFault;
+    };
+
+    Fault
+    GpuTLB::translateAtomic(const RequestPtr &req, ThreadContext *tc,
+                            Mode mode, int &latency)
+    {
+        bool delayedResponse;
+
+        return GpuTLB::translate(req, tc, nullptr, mode, delayedResponse,
+            false, latency);
+    }
+
+    void
+    GpuTLB::translateTiming(const RequestPtr &req, ThreadContext *tc,
+            Translation *translation, Mode mode, int &latency)
+    {
+        bool delayedResponse;
+        assert(translation);
+
+        Fault fault = GpuTLB::translate(req, tc, translation, mode,
+                                        delayedResponse, true, latency);
+
+        if (!delayedResponse)
+            translation->finish(fault, req, tc, mode);
+    }
+
+    Walker*
+    GpuTLB::getWalker()
+    {
+        return walker;
+    }
+
+
+    void
+    GpuTLB::serialize(CheckpointOut &cp) const
+    {
+    }
+
+    void
+    GpuTLB::unserialize(CheckpointIn &cp)
+    {
+    }
+
+    /**
+     * Do the TLB lookup for this coalesced request and schedule
+     * another event <TLB access latency> cycles later.
+     */
+
+    void
+    GpuTLB::issueTLBLookup(PacketPtr pkt)
+    {
+        assert(pkt);
+        assert(pkt->senderState);
+
+        Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
+                                        X86ISA::PageBytes);
+
+        TranslationState *sender_state =
+                safe_cast<TranslationState*>(pkt->senderState);
+
+        bool update_stats = !sender_state->isPrefetch;
+        ThreadContext * tmp_tc = sender_state->tc;
+
+        DPRINTF(GPUTLB, "Translation req. for virt. page addr %#x\n",
+                virt_page_addr);
+
+        int req_cnt = sender_state->reqCnt.back();
+
+        if (update_stats) {
+            stats.accessCycles -= (curTick() * req_cnt);
+            stats.localCycles -= curTick();
+            updatePageFootprint(virt_page_addr);
+            stats.globalNumTLBAccesses += req_cnt;
+        }
+
+        tlbOutcome lookup_outcome = TLB_MISS;
+        const RequestPtr &tmp_req = pkt->req;
+
+        // Access the TLB and figure out if it's a hit or a miss.
+        bool success = tlbLookup(tmp_req, tmp_tc, update_stats);
+
+        if (success) {
+            lookup_outcome = TLB_HIT;
+            // Put the entry in SenderState
+            TlbEntry *entry = lookup(tmp_req->getVaddr(), false);
+            assert(entry);
+
+            auto p = sender_state->tc->getProcessPtr();
+            sender_state->tlbEntry =
+                new TlbEntry(p->pid(), entry->vaddr, entry->paddr,
+                             false, false);
+
+            if (update_stats) {
+                // the reqCnt has an entry per level, so its size tells us
+                // which level we are in
+                sender_state->hitLevel = sender_state->reqCnt.size();
+                stats.globalNumTLBHits += req_cnt;
+            }
+        } else {
+            if (update_stats)
+                stats.globalNumTLBMisses += req_cnt;
+        }
+
+        /*
+         * We now know the TLB lookup outcome (if it's a hit or a miss), as
+         * well as the TLB access latency.
+         *
+         * We create and schedule a new TLBEvent which will help us take the
+         * appropriate actions (e.g., update TLB on a hit, send request to
+         * lower level TLB on a miss, or start a page walk if this was the
+         * last-level TLB)
+         */
+        TLBEvent *tlb_event =
+            new TLBEvent(this, virt_page_addr, lookup_outcome, pkt);
+
+        if (translationReturnEvent.count(virt_page_addr)) {
+            panic("Virtual Page Address %#x already has a return event\n",
+                  virt_page_addr);
+        }
+
+        translationReturnEvent[virt_page_addr] = tlb_event;
+        assert(tlb_event);
+
+        DPRINTF(GPUTLB, "schedule translationReturnEvent @ curTick %d\n",
+                curTick() + cyclesToTicks(Cycles(hitLatency)));
+
+        schedule(tlb_event, curTick() + cyclesToTicks(Cycles(hitLatency)));
+    }
+
+    GpuTLB::TLBEvent::TLBEvent(GpuTLB* _tlb, Addr _addr,
+        tlbOutcome tlb_outcome, PacketPtr _pkt)
+            : Event(CPU_Tick_Pri), tlb(_tlb), virtPageAddr(_addr),
+              outcome(tlb_outcome), pkt(_pkt)
+    {
+    }
+
+    /**
+     * Do Paging protection checks. If we encounter a page fault, then
+     * an assertion is fired.
+     */
+    void
+    GpuTLB::pagingProtectionChecks(ThreadContext *tc, PacketPtr pkt,
+            TlbEntry * tlb_entry, Mode mode)
+    {
+        HandyM5Reg m5Reg = tc->readMiscRegNoEffect(MISCREG_M5_REG);
+        uint32_t flags = pkt->req->getFlags();
+        bool storeCheck = flags & Request::READ_MODIFY_WRITE;
+
+        // Do paging protection checks.
+        bool inUser
+            = (m5Reg.cpl == 3 && !(flags & (CPL0FlagBit << FlagShift)));
+        CR0 cr0 = tc->readMiscRegNoEffect(MISCREG_CR0);
+
+        bool badWrite = (!tlb_entry->writable && (inUser || cr0.wp));
+
+        if ((inUser && !tlb_entry->user) ||
+            (mode == BaseMMU::Write && badWrite)) {
+            // The page must have been present to get into the TLB in
+            // the first place. We'll assume the reserved bits are
+            // fine even though we're not checking them.
+            panic("Page fault detected");
+        }
+
+        if (storeCheck && badWrite) {
+            // This would fault if this were a write, so return a page
+            // fault that reflects that happening.
+            panic("Page fault detected");
+        }
+    }
+
+    /**
+     * handleTranslationReturn is called on a TLB hit,
+     * when a TLB miss returns or when a page fault returns.
+     * The latter calls handelHit with TLB miss as tlbOutcome.
+     */
+    void
+    GpuTLB::handleTranslationReturn(Addr virt_page_addr,
+        tlbOutcome tlb_outcome, PacketPtr pkt)
+    {
+        assert(pkt);
+        Addr vaddr = pkt->req->getVaddr();
+
+        TranslationState *sender_state =
+            safe_cast<TranslationState*>(pkt->senderState);
+
+        ThreadContext *tc = sender_state->tc;
+        Mode mode = sender_state->tlbMode;
+
+        TlbEntry *local_entry, *new_entry;
+
+        if (tlb_outcome == TLB_HIT) {
+            DPRINTF(GPUTLB, "Translation Done - TLB Hit for addr %#x\n",
+                vaddr);
+            local_entry = sender_state->tlbEntry;
+        } else {
+            DPRINTF(GPUTLB, "Translation Done - TLB Miss for addr %#x\n",
+                    vaddr);
+
+            /**
+             * We are returning either from a page walk or from a hit at a
+             * lower TLB level. The senderState should be "carrying" a pointer
+             * to the correct TLBEntry.
+             */
+            new_entry = sender_state->tlbEntry;
+            assert(new_entry);
+            local_entry = new_entry;
+
+            if (allocationPolicy) {
+                DPRINTF(GPUTLB, "allocating entry w/ addr %#x\n",
+                        virt_page_addr);
+
+                local_entry = insert(virt_page_addr, *new_entry);
+            }
+
+            assert(local_entry);
+        }
+
+        /**
+         * At this point the packet carries an up-to-date tlbEntry pointer
+         * in its senderState.
+         * Next step is to do the paging protection checks.
+         */
+        DPRINTF(GPUTLB, "Entry found with vaddr %#x,  doing protection checks "
+                "while paddr was %#x.\n", local_entry->vaddr,
+                local_entry->paddr);
+
+        pagingProtectionChecks(tc, pkt, local_entry, mode);
+        int page_size = local_entry->size();
+        Addr paddr = local_entry->paddr | (vaddr & (page_size - 1));
+        DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, paddr);
+
+        // Since this packet will be sent through the cpu side port,
+        // it must be converted to a response pkt if it is not one already
+        if (pkt->isRequest()) {
+            pkt->makeTimingResponse();
+        }
+
+        pkt->req->setPaddr(paddr);
+
+        if (local_entry->uncacheable) {
+             pkt->req->setFlags(Request::UNCACHEABLE);
+        }
+
+        //send packet back to coalescer
+        cpuSidePort[0]->sendTimingResp(pkt);
+        //schedule cleanup event
+        cleanupQueue.push(virt_page_addr);
+
+        // schedule this only once per cycle.
+        // The check is required because we might have multiple translations
+        // returning the same cycle
+        // this is a maximum priority event and must be on the same cycle
+        // as the cleanup event in TLBCoalescer to avoid a race with
+        // IssueProbeEvent caused by TLBCoalescer::MemSidePort::recvReqRetry
+        if (!cleanupEvent.scheduled())
+            schedule(cleanupEvent, curTick());
+    }
+
+    /**
+     * Here we take the appropriate actions based on the result of the
+     * TLB lookup.
+     */
+    void
+    GpuTLB::translationReturn(Addr virtPageAddr, tlbOutcome outcome,
+                              PacketPtr pkt)
+    {
+        DPRINTF(GPUTLB, "Triggered TLBEvent for addr %#x\n", virtPageAddr);
+
+        assert(translationReturnEvent[virtPageAddr]);
+        assert(pkt);
+
+        TranslationState *tmp_sender_state =
+            safe_cast<TranslationState*>(pkt->senderState);
+
+        int req_cnt = tmp_sender_state->reqCnt.back();
+        bool update_stats = !tmp_sender_state->isPrefetch;
+
+
+        if (outcome == TLB_HIT) {
+            handleTranslationReturn(virtPageAddr, TLB_HIT, pkt);
+
+            if (update_stats) {
+                stats.accessCycles += (req_cnt * curTick());
+                stats.localCycles += curTick();
+            }
+
+        } else if (outcome == TLB_MISS) {
+
+            DPRINTF(GPUTLB, "This is a TLB miss\n");
+            if (update_stats) {
+                stats.accessCycles += (req_cnt*curTick());
+                stats.localCycles += curTick();
+            }
+
+            if (hasMemSidePort) {
+                // the one cyle added here represent the delay from when we get
+                // the reply back till when we propagate it to the coalescer
+                // above.
+                if (update_stats) {
+                    stats.accessCycles += (req_cnt * 1);
+                    stats.localCycles += 1;
+                }
+
+                /**
+                 * There is a TLB below. Send the coalesced request.
+                 * We actually send the very first packet of all the
+                 * pending packets for this virtual page address.
+                 */
+                if (!memSidePort[0]->sendTimingReq(pkt)) {
+                    DPRINTF(GPUTLB, "Failed sending translation request to "
+                            "lower level TLB for addr %#x\n", virtPageAddr);
+
+                    memSidePort[0]->retries.push_back(pkt);
+                } else {
+                    DPRINTF(GPUTLB, "Sent translation request to lower level "
+                            "TLB for addr %#x\n", virtPageAddr);
+                }
+            } else {
+                //this is the last level TLB. Start a page walk
+                DPRINTF(GPUTLB, "Last level TLB - start a page walk for "
+                        "addr %#x\n", virtPageAddr);
+
+                if (update_stats)
+                    stats.pageTableCycles -= (req_cnt*curTick());
+
+                TLBEvent *tlb_event = translationReturnEvent[virtPageAddr];
+                assert(tlb_event);
+                tlb_event->updateOutcome(PAGE_WALK);
+                schedule(tlb_event,
+                         curTick() + cyclesToTicks(Cycles(missLatency2)));
+            }
+        } else if (outcome == PAGE_WALK) {
+            if (update_stats)
+                stats.pageTableCycles += (req_cnt*curTick());
+
+            // Need to access the page table and update the TLB
+            DPRINTF(GPUTLB, "Doing a page walk for address %#x\n",
+                    virtPageAddr);
+
+            TranslationState *sender_state =
+                safe_cast<TranslationState*>(pkt->senderState);
+
+            Process *p = sender_state->tc->getProcessPtr();
+            Addr vaddr = pkt->req->getVaddr();
+
+            Addr alignedVaddr = p->pTable->pageAlign(vaddr);
+            assert(alignedVaddr == virtPageAddr);
+
+            const EmulationPageTable::Entry *pte = p->pTable->lookup(vaddr);
+            if (!pte && sender_state->tlbMode != BaseMMU::Execute &&
+                    p->fixupFault(vaddr)) {
+                pte = p->pTable->lookup(vaddr);
+            }
+
+            if (pte) {
+                DPRINTF(GPUTLB, "Mapping %#x to %#x\n", alignedVaddr,
+                        pte->paddr);
+
+                sender_state->tlbEntry =
+                    new TlbEntry(p->pid(), virtPageAddr, pte->paddr, false,
+                                 false);
+            } else {
+                sender_state->tlbEntry = nullptr;
+            }
+
+            handleTranslationReturn(virtPageAddr, TLB_MISS, pkt);
+        } else if (outcome == MISS_RETURN) {
+            /** we add an extra cycle in the return path of the translation
+             * requests in between the various TLB levels.
+             */
+            handleTranslationReturn(virtPageAddr, TLB_MISS, pkt);
+        } else {
+            panic("Unexpected TLB outcome %d", outcome);
+        }
+    }
+
+    void
+    GpuTLB::TLBEvent::process()
+    {
+        tlb->translationReturn(virtPageAddr, outcome, pkt);
+    }
+
+    const char*
+    GpuTLB::TLBEvent::description() const
+    {
+        return "trigger translationDoneEvent";
+    }
+
+    void
+    GpuTLB::TLBEvent::updateOutcome(tlbOutcome _outcome)
+    {
+        outcome = _outcome;
+    }
+
+    Addr
+    GpuTLB::TLBEvent::getTLBEventVaddr()
+    {
+        return virtPageAddr;
+    }
+
+    /**
+     * recvTiming receives a coalesced timing request from a TLBCoalescer
+     * and it calls issueTLBLookup()
+     * It only rejects the packet if we have exceeded the max
+     * outstanding number of requests for the TLB
+     */
+    bool
+    GpuTLB::CpuSidePort::recvTimingReq(PacketPtr pkt)
+    {
+        if (tlb->outstandingReqs < tlb->maxCoalescedReqs) {
+            tlb->issueTLBLookup(pkt);
+            // update number of outstanding translation requests
+            tlb->outstandingReqs++;
+            return true;
+         } else {
+            DPRINTF(GPUTLB, "Reached maxCoalescedReqs number %d\n",
+                    tlb->outstandingReqs);
+            return false;
+         }
+    }
+
+    /**
+     * handleFuncTranslationReturn is called on a TLB hit,
+     * when a TLB miss returns or when a page fault returns.
+     * It updates LRU, inserts the TLB entry on a miss
+     * depending on the allocation policy and does the required
+     * protection checks. It does NOT create a new packet to
+     * update the packet's addr; this is done in hsail-gpu code.
+     */
+    void
+    GpuTLB::handleFuncTranslationReturn(PacketPtr pkt, tlbOutcome tlb_outcome)
+    {
+        TranslationState *sender_state =
+            safe_cast<TranslationState*>(pkt->senderState);
+
+        ThreadContext *tc = sender_state->tc;
+        Mode mode = sender_state->tlbMode;
+        Addr vaddr = pkt->req->getVaddr();
+
+        TlbEntry *local_entry, *new_entry;
+
+        if (tlb_outcome == TLB_HIT) {
+            DPRINTF(GPUTLB, "Functional Translation Done - TLB hit for addr "
+                    "%#x\n", vaddr);
+
+            local_entry = sender_state->tlbEntry;
+        } else {
+            DPRINTF(GPUTLB, "Functional Translation Done - TLB miss for addr "
+                    "%#x\n", vaddr);
+
+            /**
+             * We are returning either from a page walk or from a hit at a
+             * lower TLB level. The senderState should be "carrying" a pointer
+             * to the correct TLBEntry.
+             */
+            new_entry = sender_state->tlbEntry;
+            assert(new_entry);
+            local_entry = new_entry;
+
+            if (allocationPolicy) {
+                Addr virt_page_addr = roundDown(vaddr, X86ISA::PageBytes);
+
+                DPRINTF(GPUTLB, "allocating entry w/ addr %#x\n",
+                        virt_page_addr);
+
+                local_entry = insert(virt_page_addr, *new_entry);
+            }
+
+            assert(local_entry);
+        }
+
+        DPRINTF(GPUTLB, "Entry found with vaddr %#x, doing protection checks "
+                "while paddr was %#x.\n", local_entry->vaddr,
+                local_entry->paddr);
+
+        /**
+         * Do paging checks if it's a normal functional access.  If it's for a
+         * prefetch, then sometimes you can try to prefetch something that
+         * won't pass protection. We don't actually want to fault becuase there
+         * is no demand access to deem this a violation.  Just put it in the
+         * TLB and it will fault if indeed a future demand access touches it in
+         * violation.
+         *
+         * This feature could be used to explore security issues around
+         * speculative memory accesses.
+         */
+        if (!sender_state->isPrefetch && sender_state->tlbEntry)
+            pagingProtectionChecks(tc, pkt, local_entry, mode);
+
+        int page_size = local_entry->size();
+        Addr paddr = local_entry->paddr | (vaddr & (page_size - 1));
+        DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, paddr);
+
+        pkt->req->setPaddr(paddr);
+
+        if (local_entry->uncacheable)
+             pkt->req->setFlags(Request::UNCACHEABLE);
+    }
+
+    // This is used for atomic translations. Need to
+    // make it all happen during the same cycle.
+    void
+    GpuTLB::CpuSidePort::recvFunctional(PacketPtr pkt)
+    {
+        TranslationState *sender_state =
+            safe_cast<TranslationState*>(pkt->senderState);
+
+        ThreadContext *tc = sender_state->tc;
+        bool update_stats = !sender_state->isPrefetch;
+
+        Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
+                                        X86ISA::PageBytes);
+
+        if (update_stats)
+            tlb->updatePageFootprint(virt_page_addr);
+
+        // do the TLB lookup without updating the stats
+        bool success = tlb->tlbLookup(pkt->req, tc, update_stats);
+        tlbOutcome tlb_outcome = success ? TLB_HIT : TLB_MISS;
+
+        // functional mode means no coalescing
+        // global metrics are the same as the local metrics
+        if (update_stats) {
+            tlb->stats.globalNumTLBAccesses++;
+
+            if (success) {
+                sender_state->hitLevel = sender_state->reqCnt.size();
+                tlb->stats.globalNumTLBHits++;
+            }
+        }
+
+        if (!success) {
+            if (update_stats)
+                tlb->stats.globalNumTLBMisses++;
+            if (tlb->hasMemSidePort) {
+                // there is a TLB below -> propagate down the TLB hierarchy
+                tlb->memSidePort[0]->sendFunctional(pkt);
+                // If no valid translation from a prefetch, then just return
+                if (sender_state->isPrefetch && !pkt->req->hasPaddr())
+                    return;
+            } else {
+                // Need to access the page table and update the TLB
+                DPRINTF(GPUTLB, "Doing a page walk for address %#x\n",
+                        virt_page_addr);
+
+                Process *p = tc->getProcessPtr();
+
+                Addr vaddr = pkt->req->getVaddr();
+
+                Addr alignedVaddr = p->pTable->pageAlign(vaddr);
+                assert(alignedVaddr == virt_page_addr);
+
+                const EmulationPageTable::Entry *pte =
+                        p->pTable->lookup(vaddr);
+                if (!pte && sender_state->tlbMode != BaseMMU::Execute &&
+                        p->fixupFault(vaddr)) {
+                    pte = p->pTable->lookup(vaddr);
+                }
+
+                if (!sender_state->isPrefetch) {
+                    // no PageFaults are permitted after
+                    // the second page table lookup
+                    assert(pte);
+
+                    DPRINTF(GPUTLB, "Mapping %#x to %#x\n", alignedVaddr,
+                            pte->paddr);
+
+                    sender_state->tlbEntry =
+                        new TlbEntry(p->pid(), virt_page_addr,
+                                     pte->paddr, false, false);
+                } else {
+                    // If this was a prefetch, then do the normal thing if it
+                    // was a successful translation.  Otherwise, send an empty
+                    // TLB entry back so that it can be figured out as empty
+                    // and handled accordingly.
+                    if (pte) {
+                        DPRINTF(GPUTLB, "Mapping %#x to %#x\n", alignedVaddr,
+                                pte->paddr);
+
+                        sender_state->tlbEntry =
+                            new TlbEntry(p->pid(), virt_page_addr,
+                                         pte->paddr, false, false);
+                    } else {
+                        DPRINTF(GPUPrefetch, "Prefetch failed %#x\n",
+                                alignedVaddr);
+
+                        sender_state->tlbEntry = nullptr;
+
+                        return;
+                    }
+                }
+            }
+        } else {
+            DPRINTF(GPUPrefetch, "Functional Hit for vaddr %#x\n",
+                    tlb->lookup(pkt->req->getVaddr()));
+
+            TlbEntry *entry = tlb->lookup(pkt->req->getVaddr(),
+                                             update_stats);
+
+            assert(entry);
+
+            auto p = sender_state->tc->getProcessPtr();
+            sender_state->tlbEntry =
+                new TlbEntry(p->pid(), entry->vaddr, entry->paddr,
+                             false, false);
+        }
+        // This is the function that would populate pkt->req with the paddr of
+        // the translation. But if no translation happens (i.e Prefetch fails)
+        // then the early returns in the above code wiill keep this function
+        // from executing.
+        tlb->handleFuncTranslationReturn(pkt, tlb_outcome);
+    }
+
+    void
+    GpuTLB::CpuSidePort::recvReqRetry()
+    {
+        // The CPUSidePort never sends anything but replies. No retries
+        // expected.
+        panic("recvReqRetry called");
+    }
+
+    AddrRangeList
+    GpuTLB::CpuSidePort::getAddrRanges() const
+    {
+        // currently not checked by the requestor
+        AddrRangeList ranges;
+
+        return ranges;
+    }
+
+    /**
+     * MemSidePort receives the packet back.
+     * We need to call the handleTranslationReturn
+     * and propagate up the hierarchy.
+     */
+    bool
+    GpuTLB::MemSidePort::recvTimingResp(PacketPtr pkt)
+    {
+        Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
+                                        X86ISA::PageBytes);
+
+        DPRINTF(GPUTLB, "MemSidePort recvTiming for virt_page_addr %#x\n",
+                virt_page_addr);
+
+        TLBEvent *tlb_event = tlb->translationReturnEvent[virt_page_addr];
+        assert(tlb_event);
+        assert(virt_page_addr == tlb_event->getTLBEventVaddr());
+
+        tlb_event->updateOutcome(MISS_RETURN);
+        tlb->schedule(tlb_event, curTick()+tlb->clockPeriod());
+
+        return true;
+    }
+
+    void
+    GpuTLB::MemSidePort::recvReqRetry()
+    {
+        // No retries should reach the TLB. The retries
+        // should only reach the TLBCoalescer.
+        panic("recvReqRetry called");
+    }
+
+    void
+    GpuTLB::cleanup()
+    {
+        while (!cleanupQueue.empty()) {
+            Addr cleanup_addr = cleanupQueue.front();
+            cleanupQueue.pop();
+
+            // delete TLBEvent
+            TLBEvent * old_tlb_event = translationReturnEvent[cleanup_addr];
+            delete old_tlb_event;
+            translationReturnEvent.erase(cleanup_addr);
+
+            // update number of outstanding requests
+            outstandingReqs--;
+        }
+
+        /** the higher level coalescer should retry if it has
+         * any pending requests.
+         */
+        for (int i = 0; i < cpuSidePort.size(); ++i) {
+            cpuSidePort[i]->sendRetryReq();
+        }
+    }
+
+    void
+    GpuTLB::updatePageFootprint(Addr virt_page_addr)
+    {
+
+        std::pair<AccessPatternTable::iterator, bool> ret;
+
+        AccessInfo tmp_access_info;
+        tmp_access_info.lastTimeAccessed = 0;
+        tmp_access_info.accessesPerPage = 0;
+        tmp_access_info.totalReuseDistance = 0;
+        tmp_access_info.sumDistance = 0;
+        tmp_access_info.meanDistance = 0;
+
+        ret = TLBFootprint.insert(
+            AccessPatternTable::value_type(virt_page_addr, tmp_access_info));
+
+        bool first_page_access = ret.second;
+
+        if (first_page_access) {
+            stats.numUniquePages++;
+        } else  {
+            int accessed_before;
+            accessed_before  = curTick() - ret.first->second.lastTimeAccessed;
+            ret.first->second.totalReuseDistance += accessed_before;
+        }
+
+        ret.first->second.accessesPerPage++;
+        ret.first->second.lastTimeAccessed = curTick();
+
+        if (accessDistance) {
+            ret.first->second.localTLBAccesses
+                .push_back(stats.localNumTLBAccesses.value());
+        }
+    }
+
+    void
+    GpuTLB::exitCallback()
+    {
+        std::ostream *page_stat_file = nullptr;
+
+        if (accessDistance) {
+
+            // print per page statistics to a separate file (.csv format)
+            // simout is the gem5 output directory (default is m5out or the one
+            // specified with -d
+            page_stat_file = simout.create(name().c_str())->stream();
+
+            // print header
+            *page_stat_file
+                << "page,max_access_distance,mean_access_distance, "
+                << "stddev_distance" << std::endl;
+        }
+
+        // update avg. reuse distance footprint
+        unsigned int sum_avg_reuse_distance_per_page = 0;
+
+        // iterate through all pages seen by this TLB
+        for (auto &iter : TLBFootprint) {
+            sum_avg_reuse_distance_per_page += iter.second.totalReuseDistance /
+                                               iter.second.accessesPerPage;
+
+            if (accessDistance) {
+                unsigned int tmp = iter.second.localTLBAccesses[0];
+                unsigned int prev = tmp;
+
+                for (int i = 0; i < iter.second.localTLBAccesses.size(); ++i) {
+                    if (i) {
+                        tmp = prev + 1;
+                    }
+
+                    prev = iter.second.localTLBAccesses[i];
+                    // update the localTLBAccesses value
+                    // with the actual differece
+                    iter.second.localTLBAccesses[i] -= tmp;
+                    // compute the sum of AccessDistance per page
+                    // used later for mean
+                    iter.second.sumDistance +=
+                        iter.second.localTLBAccesses[i];
+                }
+
+                iter.second.meanDistance =
+                    iter.second.sumDistance / iter.second.accessesPerPage;
+
+                // compute std_dev and max  (we need a second round because we
+                // need to know the mean value
+                unsigned int max_distance = 0;
+                unsigned int stddev_distance = 0;
+
+                for (int i = 0; i < iter.second.localTLBAccesses.size(); ++i) {
+                    unsigned int tmp_access_distance =
+                        iter.second.localTLBAccesses[i];
+
+                    if (tmp_access_distance > max_distance) {
+                        max_distance = tmp_access_distance;
+                    }
+
+                    unsigned int diff =
+                        tmp_access_distance - iter.second.meanDistance;
+                    stddev_distance += pow(diff, 2);
+
+                }
+
+                stddev_distance =
+                    sqrt(stddev_distance/iter.second.accessesPerPage);
+
+                if (page_stat_file) {
+                    *page_stat_file << std::hex << iter.first << ",";
+                    *page_stat_file << std::dec << max_distance << ",";
+                    *page_stat_file << std::dec << iter.second.meanDistance
+                                    << ",";
+                    *page_stat_file << std::dec << stddev_distance;
+                    *page_stat_file << std::endl;
+                }
+
+                // erase the localTLBAccesses array
+                iter.second.localTLBAccesses.clear();
+            }
+        }
+
+        if (!TLBFootprint.empty()) {
+            stats.avgReuseDistance =
+                sum_avg_reuse_distance_per_page / TLBFootprint.size();
+        }
+
+        //clear the TLBFootprint map
+        TLBFootprint.clear();
+    }
+
+    GpuTLB::GpuTLBStats::GpuTLBStats(statistics::Group *parent)
+        : statistics::Group(parent),
+          ADD_STAT(localNumTLBAccesses, "Number of TLB accesses"),
+          ADD_STAT(localNumTLBHits, "Number of TLB hits"),
+          ADD_STAT(localNumTLBMisses, "Number of TLB misses"),
+          ADD_STAT(localTLBMissRate, "TLB miss rate"),
+          ADD_STAT(globalNumTLBAccesses, "Number of TLB accesses"),
+          ADD_STAT(globalNumTLBHits, "Number of TLB hits"),
+          ADD_STAT(globalNumTLBMisses, "Number of TLB misses"),
+          ADD_STAT(globalTLBMissRate, "TLB miss rate"),
+          ADD_STAT(accessCycles, "Cycles spent accessing this TLB level"),
+          ADD_STAT(pageTableCycles, "Cycles spent accessing the page table"),
+          ADD_STAT(numUniquePages, "Number of unique pages touched"),
+          ADD_STAT(localCycles, "Number of cycles spent in queue for all "
+                   "incoming reqs"),
+          ADD_STAT(localLatency, "Avg. latency over incoming coalesced reqs"),
+          ADD_STAT(avgReuseDistance, "avg. reuse distance over all pages (in "
+                   "ticks)")
+    {
+        localLatency = localCycles / localNumTLBAccesses;
+
+        localTLBMissRate = 100 * localNumTLBMisses / localNumTLBAccesses;
+        globalTLBMissRate = 100 * globalNumTLBMisses / globalNumTLBAccesses;
+    }
+} // namespace X86ISA
+} // namespace gem5
diff --git a/src/arch/amdgpu/common/tlb.hh b/src/arch/amdgpu/common/tlb.hh
new file mode 100644
index 0000000..944c0ac
--- /dev/null
+++ b/src/arch/amdgpu/common/tlb.hh
@@ -0,0 +1,445 @@
+/*
+ * Copyright (c) 2011-2015 Advanced Micro Devices, Inc.
+ * All rights reserved.
+ *
+ * For use for simulation and test purposes only
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * 3. Neither the name of the copyright holder nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __GPU_TLB_HH__
+#define __GPU_TLB_HH__
+
+#include <fstream>
+#include <list>
+#include <queue>
+#include <string>
+#include <vector>
+
+#include "arch/generic/tlb.hh"
+#include "arch/x86/pagetable.hh"
+#include "arch/x86/pagetable_walker.hh"
+#include "arch/x86/regs/segment.hh"
+#include "base/callback.hh"
+#include "base/logging.hh"
+#include "base/statistics.hh"
+#include "base/stats/group.hh"
+#include "gpu-compute/compute_unit.hh"
+#include "mem/port.hh"
+#include "mem/request.hh"
+#include "params/X86GPUTLB.hh"
+#include "sim/clocked_object.hh"
+#include "sim/sim_object.hh"
+
+namespace gem5
+{
+
+class BaseTLB;
+class Packet;
+class ThreadContext;
+
+namespace X86ISA
+{
+    class GpuTLB : public ClockedObject
+    {
+      protected:
+        friend class Walker;
+
+        typedef std::list<TlbEntry*> EntryList;
+
+        uint32_t configAddress;
+
+      public:
+        typedef X86GPUTLBParams Params;
+        GpuTLB(const Params &p);
+        ~GpuTLB();
+
+        typedef enum BaseMMU::Mode Mode;
+
+        class Translation
+        {
+          public:
+            virtual ~Translation() { }
+
+            /**
+             * Signal that the translation has been delayed due to a hw page
+             * table walk.
+             */
+            virtual void markDelayed() = 0;
+
+            /**
+             * The memory for this object may be dynamically allocated, and it
+             * may be responsible for cleaning itslef up which will happen in
+             * this function. Once it's called the object is no longer valid.
+             */
+            virtual void finish(Fault fault, const RequestPtr &req,
+                                ThreadContext *tc, Mode mode) = 0;
+        };
+
+        void dumpAll();
+        TlbEntry *lookup(Addr va, bool update_lru=true);
+        void setConfigAddress(uint32_t addr);
+
+      protected:
+        EntryList::iterator lookupIt(Addr va, bool update_lru=true);
+        Walker *walker;
+
+      public:
+        Walker *getWalker();
+        void invalidateAll();
+        void invalidateNonGlobal();
+        void demapPage(Addr va, uint64_t asn);
+
+      protected:
+        int size;
+        int assoc;
+        int numSets;
+
+        /**
+         *  true if this is a fully-associative TLB
+         */
+        bool FA;
+        Addr setMask;
+
+        /**
+         * Allocation Policy: true if we always allocate on a hit, false
+         * otherwise. Default is true.
+         */
+        bool allocationPolicy;
+
+        /**
+         * if true, then this is not the last level TLB
+         */
+        bool hasMemSidePort;
+
+        /**
+         * Print out accessDistance stats. One stat file
+         * per TLB.
+         */
+        bool accessDistance;
+
+        std::vector<TlbEntry> tlb;
+
+        /*
+         * It's a per-set list. As long as we have not reached
+         * the full capacity of the given set, grab an entry from
+         * the freeList.
+         */
+        std::vector<EntryList> freeList;
+
+        /**
+         * An entryList per set is the equivalent of an LRU stack;
+         * it's used to guide replacement decisions. The head of the list
+         * contains the MRU TLB entry of the given set. If the freeList
+         * for this set is empty, the last element of the list
+         * is evicted (i.e., dropped on the floor).
+         */
+        std::vector<EntryList> entryList;
+
+        Fault translateInt(bool read, const RequestPtr &req,
+                           ThreadContext *tc);
+
+        Fault translate(const RequestPtr &req, ThreadContext *tc,
+                Translation *translation, Mode mode, bool &delayedResponse,
+                bool timing, int &latency);
+
+      public:
+        // latencies for a TLB hit, miss and page fault
+        int hitLatency;
+        int missLatency1;
+        int missLatency2;
+
+        void updatePageFootprint(Addr virt_page_addr);
+        void printAccessPattern();
+
+
+        Fault translateAtomic(const RequestPtr &req, ThreadContext *tc,
+                              Mode mode, int &latency);
+
+        void translateTiming(const RequestPtr &req, ThreadContext *tc,
+                             Translation *translation, Mode mode,
+                             int &latency);
+
+        Tick doMmuRegRead(ThreadContext *tc, Packet *pkt);
+        Tick doMmuRegWrite(ThreadContext *tc, Packet *pkt);
+
+        TlbEntry *insert(Addr vpn, TlbEntry &entry);
+
+        // Checkpointing
+        virtual void serialize(CheckpointOut& cp) const override;
+        virtual void unserialize(CheckpointIn& cp) override;
+        void issueTranslation();
+        enum tlbOutcome {TLB_HIT, TLB_MISS, PAGE_WALK, MISS_RETURN};
+        bool tlbLookup(const RequestPtr &req,
+                       ThreadContext *tc, bool update_stats);
+
+        void handleTranslationReturn(Addr addr, tlbOutcome outcome,
+                                     PacketPtr pkt);
+
+        void handleFuncTranslationReturn(PacketPtr pkt, tlbOutcome outcome);
+
+        void pagingProtectionChecks(ThreadContext *tc, PacketPtr pkt,
+                                    TlbEntry *tlb_entry, Mode mode);
+
+        void updatePhysAddresses(Addr virt_page_addr, TlbEntry *tlb_entry,
+                                 Addr phys_page_addr);
+
+        void issueTLBLookup(PacketPtr pkt);
+
+        // CpuSidePort is the TLB Port closer to the CPU/CU side
+        class CpuSidePort : public ResponsePort
+        {
+          public:
+            CpuSidePort(const std::string &_name, GpuTLB * gpu_TLB,
+                        PortID _index)
+                : ResponsePort(_name, gpu_TLB), tlb(gpu_TLB), index(_index) { }
+
+          protected:
+            GpuTLB *tlb;
+            int index;
+
+            virtual bool recvTimingReq(PacketPtr pkt);
+            virtual Tick recvAtomic(PacketPtr pkt) { return 0; }
+            virtual void recvFunctional(PacketPtr pkt);
+            virtual void recvRangeChange() { }
+            virtual void recvReqRetry();
+            virtual void recvRespRetry() { panic("recvRespRetry called"); }
+            virtual AddrRangeList getAddrRanges() const;
+        };
+
+        /**
+         * MemSidePort is the TLB Port closer to the memory side
+         * If this is a last level TLB then this port will not be connected.
+         *
+         * Future action item: if we ever do real page walks, then this port
+         * should be connected to a RubyPort.
+         */
+        class MemSidePort : public RequestPort
+        {
+          public:
+            MemSidePort(const std::string &_name, GpuTLB * gpu_TLB,
+                        PortID _index)
+                : RequestPort(_name, gpu_TLB), tlb(gpu_TLB), index(_index) { }
+
+            std::deque<PacketPtr> retries;
+
+          protected:
+            GpuTLB *tlb;
+            int index;
+
+            virtual bool recvTimingResp(PacketPtr pkt);
+            virtual Tick recvAtomic(PacketPtr pkt) { return 0; }
+            virtual void recvFunctional(PacketPtr pkt) { }
+            virtual void recvRangeChange() { }
+            virtual void recvReqRetry();
+        };
+
+        // TLB ports on the cpu Side
+        std::vector<CpuSidePort*> cpuSidePort;
+        // TLB ports on the memory side
+        std::vector<MemSidePort*> memSidePort;
+
+        Port &getPort(const std::string &if_name,
+                      PortID idx=InvalidPortID) override;
+
+        /**
+         * TLB TranslationState: this currently is a somewhat bastardization of
+         * the usage of SenderState, whereby the receiver of a packet is not
+         * usually supposed to need to look at the contents of the senderState,
+         * you're really only supposed to look at what you pushed on, pop it
+         * off, and send it back.
+         *
+         * However, since there is state that we want to pass to the TLBs using
+         * the send/recv Timing/Functional/etc. APIs, which don't allow for new
+         * arguments, we need a common TLB senderState to pass between TLBs,
+         * both "forwards" and "backwards."
+         *
+         * So, basically, the rule is that any packet received by a TLB port
+         * (cpuside OR memside) must be safely castable to a TranslationState.
+         */
+
+        struct TranslationState : public Packet::SenderState
+        {
+            // TLB mode, read or write
+            Mode tlbMode;
+            // Thread context associated with this req
+            ThreadContext *tc;
+
+            /*
+            * TLB entry to be populated and passed back and filled in
+            * previous TLBs.  Equivalent to the data cache concept of
+            * "data return."
+            */
+            TlbEntry *tlbEntry;
+            // Is this a TLB prefetch request?
+            bool isPrefetch;
+            // When was the req for this translation issued
+            uint64_t issueTime;
+            // Remember where this came from
+            std::vector<ResponsePort*>ports;
+
+            // keep track of #uncoalesced reqs per packet per TLB level;
+            // reqCnt per level >= reqCnt higher level
+            std::vector<int> reqCnt;
+            // TLB level this packet hit in; 0 if it hit in the page table
+            int hitLevel;
+            Packet::SenderState *saved;
+
+            TranslationState(Mode tlb_mode, ThreadContext *_tc,
+                             bool is_prefetch=false,
+                             Packet::SenderState *_saved=nullptr)
+                : tlbMode(tlb_mode), tc(_tc), tlbEntry(nullptr),
+                  isPrefetch(is_prefetch), issueTime(0),
+                  hitLevel(0),saved(_saved) { }
+        };
+
+        // maximum number of permitted coalesced requests per cycle
+        int maxCoalescedReqs;
+
+        // Current number of outstandings coalesced requests.
+        // Should be <= maxCoalescedReqs
+        int outstandingReqs;
+
+        /**
+         * A TLBEvent is scheduled after the TLB lookup and helps us take the
+         * appropriate actions:
+         *  (e.g., update TLB on a hit,
+         *  send request to lower level TLB on a miss,
+         *  or start a page walk if this was the last-level TLB).
+         */
+        void translationReturn(Addr virtPageAddr, tlbOutcome outcome,
+                               PacketPtr pkt);
+
+        class TLBEvent : public Event
+        {
+            private:
+                GpuTLB *tlb;
+                Addr virtPageAddr;
+                /**
+                 * outcome can be TLB_HIT, TLB_MISS, or PAGE_WALK
+                 */
+                tlbOutcome outcome;
+                PacketPtr pkt;
+
+            public:
+                TLBEvent(GpuTLB *_tlb, Addr _addr, tlbOutcome outcome,
+                        PacketPtr _pkt);
+
+                void process();
+                const char *description() const;
+
+                // updateOutcome updates the tlbOutcome of a TLBEvent
+                void updateOutcome(tlbOutcome _outcome);
+                Addr getTLBEventVaddr();
+        };
+
+        std::unordered_map<Addr, TLBEvent*> translationReturnEvent;
+
+        // this FIFO queue keeps track of the virt. page addresses
+        // that are pending cleanup
+        std::queue<Addr> cleanupQueue;
+
+        // the cleanupEvent is scheduled after a TLBEvent triggers in order to
+        // free memory and do the required clean-up
+        void cleanup();
+
+        EventFunctionWrapper cleanupEvent;
+
+        /**
+         * This hash map will use the virtual page address as a key
+         * and will keep track of total number of accesses per page
+         */
+
+        struct AccessInfo
+        {
+            unsigned int lastTimeAccessed; // last access to this page
+            unsigned int accessesPerPage;
+            // need to divide it by accessesPerPage at the end
+            unsigned int totalReuseDistance;
+
+            /**
+             * The field below will help us compute the access distance,
+             * that is the number of (coalesced) TLB accesses that
+             * happened in between each access to this page
+             *
+             * localTLBAccesses[x] is the value of localTLBNumAccesses
+             * when the page <Addr> was accessed for the <x>th time
+             */
+            std::vector<unsigned int> localTLBAccesses;
+            unsigned int sumDistance;
+            unsigned int meanDistance;
+        };
+
+        typedef std::unordered_map<Addr, AccessInfo> AccessPatternTable;
+        AccessPatternTable TLBFootprint;
+
+        // Called at the end of simulation to dump page access stats.
+        void exitCallback();
+
+        EventFunctionWrapper exitEvent;
+
+      protected:
+        struct GpuTLBStats : public statistics::Group
+        {
+            GpuTLBStats(statistics::Group *parent);
+
+            // local_stats are as seen from the TLB
+            // without taking into account coalescing
+            statistics::Scalar localNumTLBAccesses;
+            statistics::Scalar localNumTLBHits;
+            statistics::Scalar localNumTLBMisses;
+            statistics::Formula localTLBMissRate;
+
+            // global_stats are as seen from the
+            // CU's perspective taking into account
+            // all coalesced requests.
+            statistics::Scalar globalNumTLBAccesses;
+            statistics::Scalar globalNumTLBHits;
+            statistics::Scalar globalNumTLBMisses;
+            statistics::Formula globalTLBMissRate;
+
+            // from the CU perspective (global)
+            statistics::Scalar accessCycles;
+            // from the CU perspective (global)
+            statistics::Scalar pageTableCycles;
+            statistics::Scalar numUniquePages;
+            // from the perspective of this TLB
+            statistics::Scalar localCycles;
+            // from the perspective of this TLB
+            statistics::Formula localLatency;
+            // I take the avg. per page and then
+            // the avg. over all pages.
+            statistics::Scalar avgReuseDistance;
+        } stats;
+    };
+}
+
+using GpuTranslationState = X86ISA::GpuTLB::TranslationState;
+
+} // namespace gem5
+
+#endif // __GPU_TLB_HH__
diff --git a/src/arch/amdgpu/common/tlb_coalescer.cc b/src/arch/amdgpu/common/tlb_coalescer.cc
new file mode 100644
index 0000000..507d0e2
--- /dev/null
+++ b/src/arch/amdgpu/common/tlb_coalescer.cc
@@ -0,0 +1,539 @@
+/*
+ * Copyright (c) 2011-2015 Advanced Micro Devices, Inc.
+ * All rights reserved.
+ *
+ * For use for simulation and test purposes only
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * 3. Neither the name of the copyright holder nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "arch/amdgpu/common/tlb_coalescer.hh"
+
+#include <cstring>
+
+#include "arch/x86/page_size.hh"
+#include "base/logging.hh"
+#include "debug/GPUTLB.hh"
+#include "sim/process.hh"
+
+namespace gem5
+{
+
+TLBCoalescer::TLBCoalescer(const Params &p)
+    : ClockedObject(p),
+      TLBProbesPerCycle(p.probesPerCycle),
+      coalescingWindow(p.coalescingWindow),
+      disableCoalescing(p.disableCoalescing),
+      probeTLBEvent([this]{ processProbeTLBEvent(); },
+                    "Probe the TLB below",
+                    false, Event::CPU_Tick_Pri),
+      cleanupEvent([this]{ processCleanupEvent(); },
+                   "Cleanup issuedTranslationsTable hashmap",
+                   false, Event::Maximum_Pri),
+      stats(this)
+{
+    // create the response ports based on the number of connected ports
+    for (size_t i = 0; i < p.port_cpu_side_ports_connection_count; ++i) {
+        cpuSidePort.push_back(new CpuSidePort(csprintf("%s-port%d", name(), i),
+                                              this, i));
+    }
+
+    // create the request ports based on the number of connected ports
+    for (size_t i = 0; i < p.port_mem_side_ports_connection_count; ++i) {
+        memSidePort.push_back(new MemSidePort(csprintf("%s-port%d", name(), i),
+                                              this, i));
+    }
+}
+
+Port &
+TLBCoalescer::getPort(const std::string &if_name, PortID idx)
+{
+    if (if_name == "cpu_side_ports") {
+        if (idx >= static_cast<PortID>(cpuSidePort.size())) {
+            panic("TLBCoalescer::getPort: unknown index %d\n", idx);
+        }
+
+        return *cpuSidePort[idx];
+    } else  if (if_name == "mem_side_ports") {
+        if (idx >= static_cast<PortID>(memSidePort.size())) {
+            panic("TLBCoalescer::getPort: unknown index %d\n", idx);
+        }
+
+        return *memSidePort[idx];
+    } else {
+        panic("TLBCoalescer::getPort: unknown port %s\n", if_name);
+    }
+}
+
+/*
+ * This method returns true if the <incoming_pkt>
+ * can be coalesced with <coalesced_pkt> and false otherwise.
+ * A given set of rules is checked.
+ * The rules can potentially be modified based on the TLB level.
+ */
+bool
+TLBCoalescer::canCoalesce(PacketPtr incoming_pkt, PacketPtr coalesced_pkt)
+{
+    if (disableCoalescing)
+        return false;
+
+    GpuTranslationState *incoming_state =
+      safe_cast<GpuTranslationState*>(incoming_pkt->senderState);
+
+    GpuTranslationState *coalesced_state =
+     safe_cast<GpuTranslationState*>(coalesced_pkt->senderState);
+
+    // Rule 1: Coalesce requests only if they
+    // fall within the same virtual page
+    Addr incoming_virt_page_addr = roundDown(incoming_pkt->req->getVaddr(),
+                                             X86ISA::PageBytes);
+
+    Addr coalesced_virt_page_addr = roundDown(coalesced_pkt->req->getVaddr(),
+                                              X86ISA::PageBytes);
+
+    if (incoming_virt_page_addr != coalesced_virt_page_addr)
+        return false;
+
+    //* Rule 2: Coalesce requests only if they
+    // share a TLB Mode, i.e. they are both read
+    // or write requests.
+    BaseMMU::Mode incoming_mode = incoming_state->tlbMode;
+    BaseMMU::Mode coalesced_mode = coalesced_state->tlbMode;
+
+    if (incoming_mode != coalesced_mode)
+        return false;
+
+    // when we can coalesce a packet update the reqCnt
+    // that is the number of packets represented by
+    // this coalesced packet
+    if (!incoming_state->isPrefetch)
+        coalesced_state->reqCnt.back() += incoming_state->reqCnt.back();
+
+    return true;
+}
+
+/*
+ * We need to update the physical addresses of all the translation requests
+ * that were coalesced into the one that just returned.
+ */
+void
+TLBCoalescer::updatePhysAddresses(PacketPtr pkt)
+{
+    Addr virt_page_addr = roundDown(pkt->req->getVaddr(), X86ISA::PageBytes);
+
+    DPRINTF(GPUTLB, "Update phys. addr. for %d coalesced reqs for page %#x\n",
+            issuedTranslationsTable[virt_page_addr].size(), virt_page_addr);
+
+    GpuTranslationState *sender_state =
+        safe_cast<GpuTranslationState*>(pkt->senderState);
+
+    TheISA::TlbEntry *tlb_entry = sender_state->tlbEntry;
+    assert(tlb_entry);
+    Addr first_entry_vaddr = tlb_entry->vaddr;
+    Addr first_entry_paddr = tlb_entry->paddr;
+    int page_size = tlb_entry->size();
+    bool uncacheable = tlb_entry->uncacheable;
+    int first_hit_level = sender_state->hitLevel;
+
+    // Get the physical page address of the translated request
+    // Using the page_size specified in the TLBEntry allows us
+    // to support different page sizes.
+    Addr phys_page_paddr = pkt->req->getPaddr();
+    phys_page_paddr &= ~(page_size - 1);
+
+    for (int i = 0; i < issuedTranslationsTable[virt_page_addr].size(); ++i) {
+        PacketPtr local_pkt = issuedTranslationsTable[virt_page_addr][i];
+        GpuTranslationState *sender_state =
+            safe_cast<GpuTranslationState*>(
+                    local_pkt->senderState);
+
+        // we are sending the packet back, so pop the reqCnt associated
+        // with this level in the TLB hiearchy
+        if (!sender_state->isPrefetch)
+            sender_state->reqCnt.pop_back();
+
+        /*
+         * Only the first packet from this coalesced request has been
+         * translated. Grab the translated phys. page addr and update the
+         * physical addresses of the remaining packets with the appropriate
+         * page offsets.
+         */
+        if (i) {
+            Addr paddr = phys_page_paddr;
+            paddr |= (local_pkt->req->getVaddr() & (page_size - 1));
+            local_pkt->req->setPaddr(paddr);
+
+            if (uncacheable)
+                local_pkt->req->setFlags(Request::UNCACHEABLE);
+
+            // update senderState->tlbEntry, so we can insert
+            // the correct TLBEentry in the TLBs above.
+            auto p = sender_state->tc->getProcessPtr();
+            sender_state->tlbEntry =
+                new TheISA::TlbEntry(p->pid(), first_entry_vaddr,
+                    first_entry_paddr, false, false);
+
+            // update the hitLevel for all uncoalesced reqs
+            // so that each packet knows where it hit
+            // (used for statistics in the CUs)
+            sender_state->hitLevel = first_hit_level;
+        }
+
+        ResponsePort *return_port = sender_state->ports.back();
+        sender_state->ports.pop_back();
+
+        // Translation is done - Convert to a response pkt if necessary and
+        // send the translation back
+        if (local_pkt->isRequest()) {
+            local_pkt->makeTimingResponse();
+        }
+
+        return_port->sendTimingResp(local_pkt);
+    }
+
+    // schedule clean up for end of this cycle
+    // This is a maximum priority event and must be on
+    // the same cycle as GPUTLB cleanup event to prevent
+    // race conditions with an IssueProbeEvent caused by
+    // MemSidePort::recvReqRetry
+    cleanupQueue.push(virt_page_addr);
+
+    if (!cleanupEvent.scheduled())
+        schedule(cleanupEvent, curTick());
+}
+
+// Receive translation requests, create a coalesced request,
+// and send them to the TLB (TLBProbesPerCycle)
+bool
+TLBCoalescer::CpuSidePort::recvTimingReq(PacketPtr pkt)
+{
+    // first packet of a coalesced request
+    PacketPtr first_packet = nullptr;
+    // true if we are able to do coalescing
+    bool didCoalesce = false;
+    // number of coalesced reqs for a given window
+    int coalescedReq_cnt = 0;
+
+    GpuTranslationState *sender_state =
+        safe_cast<GpuTranslationState*>(pkt->senderState);
+
+    // push back the port to remember the path back
+    sender_state->ports.push_back(this);
+
+    bool update_stats = !sender_state->isPrefetch;
+
+    if (update_stats) {
+        // if reqCnt is empty then this packet does not represent
+        // multiple uncoalesced reqs(pkts) but just a single pkt.
+        // If it does though then the reqCnt for each level in the
+        // hierarchy accumulates the total number of reqs this packet
+        // represents
+        int req_cnt = 1;
+
+        if (!sender_state->reqCnt.empty())
+            req_cnt = sender_state->reqCnt.back();
+
+        sender_state->reqCnt.push_back(req_cnt);
+
+        // update statistics
+        coalescer->stats.uncoalescedAccesses++;
+        req_cnt = sender_state->reqCnt.back();
+        DPRINTF(GPUTLB, "receiving pkt w/ req_cnt %d\n", req_cnt);
+        coalescer->stats.queuingCycles -= (curTick() * req_cnt);
+        coalescer->stats.localqueuingCycles -= curTick();
+    }
+
+    // FIXME if you want to coalesce not based on the issueTime
+    // of the packets (i.e., from the compute unit's perspective)
+    // but based on when they reached this coalescer then
+    // remove the following if statement and use curTick() or
+    // coalescingWindow for the tick_index.
+    if (!sender_state->issueTime)
+       sender_state->issueTime = curTick();
+
+    // The tick index is used as a key to the coalescerFIFO hashmap.
+    // It is shared by all candidates that fall within the
+    // given coalescingWindow.
+    int64_t tick_index = sender_state->issueTime / coalescer->coalescingWindow;
+
+    if (coalescer->coalescerFIFO.count(tick_index)) {
+        coalescedReq_cnt = coalescer->coalescerFIFO[tick_index].size();
+    }
+
+    // see if we can coalesce the incoming pkt with another
+    // coalesced request with the same tick_index
+    for (int i = 0; i < coalescedReq_cnt; ++i) {
+        first_packet = coalescer->coalescerFIFO[tick_index][i][0];
+
+        if (coalescer->canCoalesce(pkt, first_packet)) {
+            coalescer->coalescerFIFO[tick_index][i].push_back(pkt);
+
+            DPRINTF(GPUTLB, "Coalesced req %i w/ tick_index %d has %d reqs\n",
+                    i, tick_index,
+                    coalescer->coalescerFIFO[tick_index][i].size());
+
+            didCoalesce = true;
+            break;
+        }
+    }
+
+    // if this is the first request for this tick_index
+    // or we did not manage to coalesce, update stats
+    // and make necessary allocations.
+    if (!coalescedReq_cnt || !didCoalesce) {
+        if (update_stats)
+            coalescer->stats.coalescedAccesses++;
+
+        std::vector<PacketPtr> new_array;
+        new_array.push_back(pkt);
+        coalescer->coalescerFIFO[tick_index].push_back(new_array);
+
+        DPRINTF(GPUTLB, "coalescerFIFO[%d] now has %d coalesced reqs after "
+                "push\n", tick_index,
+                coalescer->coalescerFIFO[tick_index].size());
+    }
+
+    //schedule probeTLBEvent next cycle to send the
+    //coalesced requests to the TLB
+    if (!coalescer->probeTLBEvent.scheduled()) {
+        coalescer->schedule(coalescer->probeTLBEvent,
+                curTick() + coalescer->clockPeriod());
+    }
+
+    return true;
+}
+
+void
+TLBCoalescer::CpuSidePort::recvReqRetry()
+{
+    panic("recvReqRetry called");
+}
+
+void
+TLBCoalescer::CpuSidePort::recvFunctional(PacketPtr pkt)
+{
+
+    GpuTranslationState *sender_state =
+        safe_cast<GpuTranslationState*>(pkt->senderState);
+
+    bool update_stats = !sender_state->isPrefetch;
+
+    if (update_stats)
+        coalescer->stats.uncoalescedAccesses++;
+
+    // If there is a pending timing request for this virtual address
+    // print a warning message. This is a temporary caveat of
+    // the current simulator where atomic and timing requests can
+    // coexist. FIXME remove this check/warning in the future.
+    Addr virt_page_addr = roundDown(pkt->req->getVaddr(), X86ISA::PageBytes);
+    int map_count = coalescer->issuedTranslationsTable.count(virt_page_addr);
+
+    if (map_count) {
+        DPRINTF(GPUTLB, "Warning! Functional access to addr %#x sees timing "
+                "req. pending\n", virt_page_addr);
+    }
+
+    coalescer->memSidePort[0]->sendFunctional(pkt);
+}
+
+AddrRangeList
+TLBCoalescer::CpuSidePort::getAddrRanges() const
+{
+    // currently not checked by the requestor
+    AddrRangeList ranges;
+
+    return ranges;
+}
+
+bool
+TLBCoalescer::MemSidePort::recvTimingResp(PacketPtr pkt)
+{
+    // a translation completed and returned
+    coalescer->updatePhysAddresses(pkt);
+
+    return true;
+}
+
+void
+TLBCoalescer::MemSidePort::recvReqRetry()
+{
+    //we've receeived a retry. Schedule a probeTLBEvent
+    if (!coalescer->probeTLBEvent.scheduled())
+        coalescer->schedule(coalescer->probeTLBEvent,
+                curTick() + coalescer->clockPeriod());
+}
+
+void
+TLBCoalescer::MemSidePort::recvFunctional(PacketPtr pkt)
+{
+    fatal("Memory side recvFunctional() not implemented in TLB coalescer.\n");
+}
+
+/*
+ * Here we scan the coalescer FIFO and issue the max
+ * number of permitted probes to the TLB below. We
+ * permit bypassing of coalesced requests for the same
+ * tick_index.
+ *
+ * We do not access the next tick_index unless we've
+ * drained the previous one. The coalesced requests
+ * that are successfully sent are moved to the
+ * issuedTranslationsTable table (the table which keeps
+ * track of the outstanding reqs)
+ */
+void
+TLBCoalescer::processProbeTLBEvent()
+{
+    // number of TLB probes sent so far
+    int sent_probes = 0;
+    // rejected denotes a blocking event
+    bool rejected = false;
+
+    // It is set to true either when the recvTiming of the TLB below
+    // returns false or when there is another outstanding request for the
+    // same virt. page.
+
+    DPRINTF(GPUTLB, "triggered TLBCoalescer %s\n", __func__);
+
+    for (auto iter = coalescerFIFO.begin();
+         iter != coalescerFIFO.end() && !rejected; ) {
+        int coalescedReq_cnt = iter->second.size();
+        int i = 0;
+        int vector_index = 0;
+
+        DPRINTF(GPUTLB, "coalescedReq_cnt is %d for tick_index %d\n",
+               coalescedReq_cnt, iter->first);
+
+        while (i < coalescedReq_cnt) {
+            ++i;
+            PacketPtr first_packet = iter->second[vector_index][0];
+
+            // compute virtual page address for this request
+            Addr virt_page_addr = roundDown(first_packet->req->getVaddr(),
+                    X86ISA::PageBytes);
+
+            // is there another outstanding request for the same page addr?
+            int pending_reqs =
+                issuedTranslationsTable.count(virt_page_addr);
+
+            if (pending_reqs) {
+                DPRINTF(GPUTLB, "Cannot issue - There are pending reqs for "
+                        "page %#x\n", virt_page_addr);
+
+                ++vector_index;
+                rejected = true;
+
+                continue;
+            }
+
+            // send the coalesced request for virt_page_addr
+            if (!memSidePort[0]->sendTimingReq(first_packet)) {
+                DPRINTF(GPUTLB, "Failed to send TLB request for page %#x\n",
+                       virt_page_addr);
+
+                // No need for a retries queue since we are already buffering
+                // the coalesced request in coalescerFIFO.
+                rejected = true;
+                ++vector_index;
+            } else {
+                GpuTranslationState *tmp_sender_state =
+                    safe_cast<GpuTranslationState*>
+                    (first_packet->senderState);
+
+                bool update_stats = !tmp_sender_state->isPrefetch;
+
+                if (update_stats) {
+                    // req_cnt is total number of packets represented
+                    // by the one we just sent counting all the way from
+                    // the top of TLB hiearchy (i.e., from the CU)
+                    int req_cnt = tmp_sender_state->reqCnt.back();
+                    stats.queuingCycles += (curTick() * req_cnt);
+
+                    DPRINTF(GPUTLB, "%s sending pkt w/ req_cnt %d\n",
+                            name(), req_cnt);
+
+                    // pkt_cnt is number of packets we coalesced into the one
+                    // we just sent but only at this coalescer level
+                    int pkt_cnt = iter->second[vector_index].size();
+                    stats.localqueuingCycles += (curTick() * pkt_cnt);
+                }
+
+                DPRINTF(GPUTLB, "Successfully sent TLB request for page %#x",
+                       virt_page_addr);
+
+                //copy coalescedReq to issuedTranslationsTable
+                issuedTranslationsTable[virt_page_addr]
+                    = iter->second[vector_index];
+
+                //erase the entry of this coalesced req
+                iter->second.erase(iter->second.begin() + vector_index);
+
+                if (iter->second.empty())
+                    assert(i == coalescedReq_cnt);
+
+                sent_probes++;
+                if (sent_probes == TLBProbesPerCycle)
+                   return;
+            }
+        }
+
+        //if there are no more coalesced reqs for this tick_index
+        //erase the hash_map with the first iterator
+        if (iter->second.empty()) {
+            coalescerFIFO.erase(iter++);
+        } else {
+            ++iter;
+        }
+    }
+}
+
+void
+TLBCoalescer::processCleanupEvent()
+{
+    while (!cleanupQueue.empty()) {
+        Addr cleanup_addr = cleanupQueue.front();
+        cleanupQueue.pop();
+        issuedTranslationsTable.erase(cleanup_addr);
+
+        DPRINTF(GPUTLB, "Cleanup - Delete coalescer entry with key %#x\n",
+                cleanup_addr);
+    }
+}
+
+TLBCoalescer::TLBCoalescerStats::TLBCoalescerStats(statistics::Group *parent)
+    : statistics::Group(parent),
+      ADD_STAT(uncoalescedAccesses, "Number of uncoalesced TLB accesses"),
+      ADD_STAT(coalescedAccesses, "Number of coalesced TLB accesses"),
+      ADD_STAT(queuingCycles, "Number of cycles spent in queue"),
+      ADD_STAT(localqueuingCycles,
+               "Number of cycles spent in queue for all incoming reqs"),
+      ADD_STAT(localLatency, "Avg. latency over all incoming pkts")
+{
+    localLatency = localqueuingCycles / uncoalescedAccesses;
+}
+
+} // namespace gem5
diff --git a/src/arch/amdgpu/common/tlb_coalescer.hh b/src/arch/amdgpu/common/tlb_coalescer.hh
new file mode 100644
index 0000000..22f311c
--- /dev/null
+++ b/src/arch/amdgpu/common/tlb_coalescer.hh
@@ -0,0 +1,226 @@
+/*
+ * Copyright (c) 2011-2015 Advanced Micro Devices, Inc.
+ * All rights reserved.
+ *
+ * For use for simulation and test purposes only
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * 3. Neither the name of the copyright holder nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __TLB_COALESCER_HH__
+#define __TLB_COALESCER_HH__
+
+#include <list>
+#include <queue>
+#include <string>
+#include <vector>
+
+#include "arch/amdgpu/common/tlb.hh"
+#include "arch/generic/tlb.hh"
+#include "arch/x86/isa.hh"
+#include "arch/x86/pagetable.hh"
+#include "arch/x86/regs/segment.hh"
+#include "base/logging.hh"
+#include "base/statistics.hh"
+#include "mem/port.hh"
+#include "mem/request.hh"
+#include "params/TLBCoalescer.hh"
+#include "sim/clocked_object.hh"
+
+namespace gem5
+{
+
+class BaseTLB;
+class Packet;
+class ThreadContext;
+
+/**
+ * The TLBCoalescer is a ClockedObject sitting on the front side (CPUSide) of
+ * each TLB. It receives packets and issues coalesced requests to the
+ * TLB below it. It controls how requests are coalesced (the rules)
+ * and the permitted number of TLB probes per cycle (i.e., how many
+ * coalesced requests it feeds the TLB per cycle).
+ */
+class TLBCoalescer : public ClockedObject
+{
+  public:
+    typedef TLBCoalescerParams Params;
+    TLBCoalescer(const Params &p);
+    ~TLBCoalescer() { }
+
+    // Number of TLB probes per cycle. Parameterizable - default 2.
+    int TLBProbesPerCycle;
+
+    // Consider coalescing across that many ticks.
+    // Paraemterizable - default 1.
+    int coalescingWindow;
+
+    // Each coalesced request consists of multiple packets
+    // that all fall within the same virtual page
+    typedef std::vector<PacketPtr> coalescedReq;
+
+    // disables coalescing when true
+    bool disableCoalescing;
+
+    /*
+     * This is a hash map with <tick_index> as a key.
+     * It contains a vector of coalescedReqs per <tick_index>.
+     * Requests are buffered here until they can be issued to
+     * the TLB, at which point they are copied to the
+     * issuedTranslationsTable hash map.
+     *
+     * In terms of coalescing, we coalesce requests in a given
+     * window of x cycles by using tick_index = issueTime/x as a
+     * key, where x = coalescingWindow. issueTime is the issueTime
+     * of the pkt from the ComputeUnit's perspective, but another
+     * option is to change it to curTick(), so we coalesce based
+     * on the receive time.
+     */
+    typedef std::map<int64_t, std::vector<coalescedReq>>
+        CoalescingFIFO;
+
+    CoalescingFIFO coalescerFIFO;
+
+    /*
+     * issuedTranslationsTabler: a hash_map indexed by virtual page
+     * address. Each hash_map entry has a vector of PacketPtr associated
+     * with it denoting the different packets that share an outstanding
+     * coalesced translation request for the same virtual page.
+     *
+     * The rules that determine which requests we can coalesce are
+     * specified in the canCoalesce() method.
+     */
+    typedef std::unordered_map<Addr, coalescedReq> CoalescingTable;
+
+    CoalescingTable issuedTranslationsTable;
+
+    bool canCoalesce(PacketPtr pkt1, PacketPtr pkt2);
+    void updatePhysAddresses(PacketPtr pkt);
+
+    class CpuSidePort : public ResponsePort
+    {
+      public:
+        CpuSidePort(const std::string &_name, TLBCoalescer *tlb_coalescer,
+                    PortID _index)
+            : ResponsePort(_name, tlb_coalescer), coalescer(tlb_coalescer),
+              index(_index) { }
+
+      protected:
+        TLBCoalescer *coalescer;
+        int index;
+
+        virtual bool recvTimingReq(PacketPtr pkt);
+        virtual Tick recvAtomic(PacketPtr pkt) { return 0; }
+        virtual void recvFunctional(PacketPtr pkt);
+        virtual void recvRangeChange() { }
+        virtual void recvReqRetry();
+
+        virtual void
+        recvRespRetry()
+        {
+            fatal("recvRespRetry() is not implemented in the TLB "
+                "coalescer.\n");
+        }
+
+        virtual AddrRangeList getAddrRanges() const;
+    };
+
+    class MemSidePort : public RequestPort
+    {
+      public:
+        MemSidePort(const std::string &_name, TLBCoalescer *tlb_coalescer,
+                    PortID _index)
+            : RequestPort(_name, tlb_coalescer), coalescer(tlb_coalescer),
+              index(_index) { }
+
+        std::deque<PacketPtr> retries;
+
+      protected:
+        TLBCoalescer *coalescer;
+        int index;
+
+        virtual bool recvTimingResp(PacketPtr pkt);
+        virtual Tick recvAtomic(PacketPtr pkt) { return 0; }
+        virtual void recvFunctional(PacketPtr pkt);
+        virtual void recvRangeChange() { }
+        virtual void recvReqRetry();
+
+        virtual void
+        recvRespRetry()
+        {
+            fatal("recvRespRetry() not implemented in TLB coalescer");
+        }
+    };
+
+    // Coalescer response ports on the cpu Side
+    std::vector<CpuSidePort*> cpuSidePort;
+    // Coalescer request ports on the memory side
+    std::vector<MemSidePort*> memSidePort;
+
+    Port &getPort(const std::string &if_name,
+                  PortID idx=InvalidPortID) override;
+
+    void processProbeTLBEvent();
+    /// This event issues the TLB probes
+    EventFunctionWrapper probeTLBEvent;
+
+    void processCleanupEvent();
+    /// The cleanupEvent is scheduled after a TLBEvent triggers
+    /// in order to free memory and do the required clean-up
+    EventFunctionWrapper cleanupEvent;
+
+    // this FIFO queue keeps track of the virt. page
+    // addresses that are pending cleanup
+    std::queue<Addr> cleanupQueue;
+
+  protected:
+    struct TLBCoalescerStats : public statistics::Group
+    {
+        TLBCoalescerStats(statistics::Group *parent);
+
+        // number of packets the coalescer receives
+        statistics::Scalar uncoalescedAccesses;
+        // number packets the coalescer send to the TLB
+        statistics::Scalar coalescedAccesses;
+
+        // Number of cycles the coalesced requests spend waiting in
+        // coalescerFIFO. For each packet the coalescer receives we take into
+        // account the number of all uncoalesced requests this pkt "represents"
+        statistics::Scalar queuingCycles;
+
+        // On average how much time a request from the
+        // uncoalescedAccesses that reaches the TLB
+        // spends waiting?
+        statistics::Scalar localqueuingCycles;
+        // localqueuingCycles/uncoalescedAccesses
+        statistics::Formula localLatency;
+    } stats;
+};
+
+} // namespace gem5
+
+#endif // __TLB_COALESCER_HH__
diff --git a/src/arch/amdgpu/gcn3/SConscript b/src/arch/amdgpu/gcn3/SConscript
index e8e0002..eb309c4 100644
--- a/src/arch/amdgpu/gcn3/SConscript
+++ b/src/arch/amdgpu/gcn3/SConscript
@@ -39,15 +39,11 @@ if not env['BUILD_GPU']:
     Return()
 
 if env['TARGET_GPU_ISA'] == 'gcn3':
-    SimObject('X86GPUTLB.py', sim_objects=['X86GPUTLB', 'TLBCoalescer'])
-
     Source('decoder.cc')
     Source('insts/gpu_static_inst.cc')
     Source('insts/instructions.cc')
     Source('insts/op_encodings.cc')
     Source('isa.cc')
     Source('registers.cc')
-    Source('tlb.cc')
-    Source('tlb_coalescer.cc')
 
     DebugFlag('GCN3', 'Debug flag for GCN3 GPU ISA')
diff --git a/src/arch/amdgpu/gcn3/X86GPUTLB.py b/src/arch/amdgpu/gcn3/X86GPUTLB.py
deleted file mode 100644
index 1c7f1d0..0000000
--- a/src/arch/amdgpu/gcn3/X86GPUTLB.py
+++ /dev/null
@@ -1,76 +0,0 @@
-# Copyright (c) 2011-2015 Advanced Micro Devices, Inc.
-# All rights reserved.
-#
-# For use for simulation and test purposes only
-#
-# Redistribution and use in source and binary forms, with or without
-# modification, are permitted provided that the following conditions are met:
-#
-# 1. Redistributions of source code must retain the above copyright notice,
-# this list of conditions and the following disclaimer.
-#
-# 2. Redistributions in binary form must reproduce the above copyright notice,
-# this list of conditions and the following disclaimer in the documentation
-# and/or other materials provided with the distribution.
-#
-# 3. Neither the name of the copyright holder nor the names of its
-# contributors may be used to endorse or promote products derived from this
-# software without specific prior written permission.
-#
-# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
-# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
-# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
-# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
-# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
-# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
-# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
-# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
-# POSSIBILITY OF SUCH DAMAGE.
-
-from m5.defines import buildEnv
-from m5.params import *
-from m5.proxy import *
-
-from m5.objects.ClockedObject import ClockedObject
-from m5.SimObject import SimObject
-
-class X86GPUTLB(ClockedObject):
-    type = 'X86GPUTLB'
-    cxx_class = 'gem5::X86ISA::GpuTLB'
-    cxx_header = 'arch/amdgpu/gcn3/tlb.hh'
-    size = Param.Int(64, "TLB size (number of entries)")
-    assoc = Param.Int(64, "TLB associativity")
-
-    if buildEnv.get('FULL_SYSTEM', False):
-        walker = Param.X86PagetableWalker(X86PagetableWalker(),
-                                          "page table walker")
-
-    hitLatency = Param.Int(2, "Latency of a TLB hit")
-    missLatency1 = Param.Int(5, "Latency #1 of a TLB miss")
-    missLatency2 = Param.Int(100, "Latency #2 of a TLB miss")
-    maxOutstandingReqs = Param.Int(64, "# of maximum outstanding requests")
-    cpu_side_ports = VectorResponsePort("Ports on side closer to CPU/CU")
-    slave    = DeprecatedParam(cpu_side_ports,
-                        '`slave` is now called `cpu_side_ports`')
-    mem_side_ports = VectorRequestPort("Ports on side closer to memory")
-    master   = DeprecatedParam(mem_side_ports,
-                        '`master` is now called `mem_side_ports`')
-    allocationPolicy = Param.Bool(True, "Allocate on an access")
-    accessDistance = Param.Bool(False, "print accessDistance stats")
-
-class TLBCoalescer(ClockedObject):
-    type = 'TLBCoalescer'
-    cxx_class = 'gem5::TLBCoalescer'
-    cxx_header = 'arch/amdgpu/gcn3/tlb_coalescer.hh'
-
-    probesPerCycle = Param.Int(2, "Number of TLB probes per cycle")
-    coalescingWindow = Param.Int(1, "Permit coalescing across that many ticks")
-    cpu_side_ports = VectorResponsePort("Port on side closer to CPU/CU")
-    slave    = DeprecatedParam(cpu_side_ports,
-                        '`slave` is now called `cpu_side_ports`')
-    mem_side_ports = VectorRequestPort("Port on side closer to memory")
-    master   = DeprecatedParam(mem_side_ports,
-                        '`master` is now called `mem_side_ports`')
-    disableCoalescing = Param.Bool(False,"Dispable Coalescing")
diff --git a/src/arch/amdgpu/gcn3/gpu_isa.hh b/src/arch/amdgpu/gcn3/gpu_isa.hh
index 205f097..26cbc5c 100644
--- a/src/arch/amdgpu/gcn3/gpu_isa.hh
+++ b/src/arch/amdgpu/gcn3/gpu_isa.hh
@@ -37,8 +37,8 @@
 #include <array>
 #include <type_traits>
 
+#include "arch/amdgpu/common/tlb.hh"
 #include "arch/amdgpu/gcn3/gpu_registers.hh"
-#include "arch/amdgpu/gcn3/tlb.hh"
 #include "gpu-compute/dispatcher.hh"
 #include "gpu-compute/hsa_queue_entry.hh"
 #include "gpu-compute/misc.hh"
diff --git a/src/arch/amdgpu/gcn3/tlb.cc b/src/arch/amdgpu/gcn3/tlb.cc
deleted file mode 100644
index a6280da..0000000
--- a/src/arch/amdgpu/gcn3/tlb.cc
+++ /dev/null
@@ -1,1460 +0,0 @@
-/*
- * Copyright (c) 2011-2021 Advanced Micro Devices, Inc.
- * All rights reserved.
- *
- * For use for simulation and test purposes only
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright notice,
- * this list of conditions and the following disclaimer.
- *
- * 2. Redistributions in binary form must reproduce the above copyright notice,
- * this list of conditions and the following disclaimer in the documentation
- * and/or other materials provided with the distribution.
- *
- * 3. Neither the name of the copyright holder nor the names of its
- * contributors may be used to endorse or promote products derived from this
- * software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- *
- */
-
-#include "arch/amdgpu/gcn3/tlb.hh"
-
-#include <cmath>
-#include <cstring>
-
-#include "arch/x86/faults.hh"
-#include "arch/x86/insts/microldstop.hh"
-#include "arch/x86/page_size.hh"
-#include "arch/x86/pagetable.hh"
-#include "arch/x86/pagetable_walker.hh"
-#include "arch/x86/regs/misc.hh"
-#include "arch/x86/regs/msr.hh"
-#include "arch/x86/x86_traits.hh"
-#include "base/bitfield.hh"
-#include "base/logging.hh"
-#include "base/output.hh"
-#include "base/trace.hh"
-#include "cpu/base.hh"
-#include "cpu/thread_context.hh"
-#include "debug/GPUPrefetch.hh"
-#include "debug/GPUTLB.hh"
-#include "mem/packet_access.hh"
-#include "mem/page_table.hh"
-#include "mem/request.hh"
-#include "sim/process.hh"
-#include "sim/pseudo_inst.hh"
-
-namespace gem5
-{
-namespace X86ISA
-{
-
-    GpuTLB::GpuTLB(const Params &p)
-        : ClockedObject(p), configAddress(0), size(p.size),
-          cleanupEvent([this]{ cleanup(); }, name(), false,
-                       Event::Maximum_Pri),
-          exitEvent([this]{ exitCallback(); }, name()), stats(this)
-    {
-        assoc = p.assoc;
-        assert(assoc <= size);
-        numSets = size/assoc;
-        allocationPolicy = p.allocationPolicy;
-        hasMemSidePort = false;
-        accessDistance = p.accessDistance;
-
-        tlb.assign(size, TlbEntry());
-
-        freeList.resize(numSets);
-        entryList.resize(numSets);
-
-        for (int set = 0; set < numSets; ++set) {
-            for (int way = 0; way < assoc; ++way) {
-                int x = set * assoc + way;
-                freeList[set].push_back(&tlb.at(x));
-            }
-        }
-
-        FA = (size == assoc);
-
-        /**
-         * @warning: the set-associative version assumes you have a
-         * fixed page size of 4KB.
-         * If the page size is greather than 4KB (as defined in the
-         * X86ISA::PageBytes), then there are various issues w/ the current
-         * implementation (you'd have the same 8KB page being replicated in
-         * different sets etc)
-         */
-        setMask = numSets - 1;
-
-        maxCoalescedReqs = p.maxOutstandingReqs;
-
-        // Do not allow maxCoalescedReqs to be more than the TLB associativity
-        if (maxCoalescedReqs > assoc) {
-            maxCoalescedReqs = assoc;
-            cprintf("Forcing maxCoalescedReqs to %d (TLB assoc.) \n", assoc);
-        }
-
-        outstandingReqs = 0;
-        hitLatency = p.hitLatency;
-        missLatency1 = p.missLatency1;
-        missLatency2 = p.missLatency2;
-
-        // create the response ports based on the number of connected ports
-        for (size_t i = 0; i < p.port_cpu_side_ports_connection_count; ++i) {
-            cpuSidePort.push_back(new CpuSidePort(csprintf("%s-port%d",
-                                  name(), i), this, i));
-        }
-
-        // create the request ports based on the number of connected ports
-        for (size_t i = 0; i < p.port_mem_side_ports_connection_count; ++i) {
-            memSidePort.push_back(new MemSidePort(csprintf("%s-port%d",
-                                  name(), i), this, i));
-        }
-    }
-
-    // fixme: this is never called?
-    GpuTLB::~GpuTLB()
-    {
-        // make sure all the hash-maps are empty
-        assert(translationReturnEvent.empty());
-    }
-
-    Port &
-    GpuTLB::getPort(const std::string &if_name, PortID idx)
-    {
-        if (if_name == "cpu_side_ports") {
-            if (idx >= static_cast<PortID>(cpuSidePort.size())) {
-                panic("TLBCoalescer::getPort: unknown index %d\n", idx);
-            }
-
-            return *cpuSidePort[idx];
-        } else if (if_name == "mem_side_ports") {
-            if (idx >= static_cast<PortID>(memSidePort.size())) {
-                panic("TLBCoalescer::getPort: unknown index %d\n", idx);
-            }
-
-            hasMemSidePort = true;
-
-            return *memSidePort[idx];
-        } else {
-            panic("TLBCoalescer::getPort: unknown port %s\n", if_name);
-        }
-    }
-
-    TlbEntry*
-    GpuTLB::insert(Addr vpn, TlbEntry &entry)
-    {
-        TlbEntry *newEntry = nullptr;
-
-        /**
-         * vpn holds the virtual page address
-         * The least significant bits are simply masked
-         */
-        int set = (vpn >> PageShift) & setMask;
-
-        if (!freeList[set].empty()) {
-            newEntry = freeList[set].front();
-            freeList[set].pop_front();
-        } else {
-            newEntry = entryList[set].back();
-            entryList[set].pop_back();
-        }
-
-        *newEntry = entry;
-        newEntry->vaddr = vpn;
-        entryList[set].push_front(newEntry);
-
-        return newEntry;
-    }
-
-    GpuTLB::EntryList::iterator
-    GpuTLB::lookupIt(Addr va, bool update_lru)
-    {
-        int set = (va >> PageShift) & setMask;
-
-        if (FA) {
-            assert(!set);
-        }
-
-        auto entry = entryList[set].begin();
-        for (; entry != entryList[set].end(); ++entry) {
-            int page_size = (*entry)->size();
-
-            if ((*entry)->vaddr <= va && (*entry)->vaddr + page_size > va) {
-                DPRINTF(GPUTLB, "Matched vaddr %#x to entry starting at %#x "
-                        "with size %#x.\n", va, (*entry)->vaddr, page_size);
-
-                if (update_lru) {
-                    entryList[set].push_front(*entry);
-                    entryList[set].erase(entry);
-                    entry = entryList[set].begin();
-                }
-
-                break;
-            }
-        }
-
-        return entry;
-    }
-
-    TlbEntry*
-    GpuTLB::lookup(Addr va, bool update_lru)
-    {
-        int set = (va >> PageShift) & setMask;
-
-        auto entry = lookupIt(va, update_lru);
-
-        if (entry == entryList[set].end())
-            return nullptr;
-        else
-            return *entry;
-    }
-
-    void
-    GpuTLB::invalidateAll()
-    {
-        DPRINTF(GPUTLB, "Invalidating all entries.\n");
-
-        for (int i = 0; i < numSets; ++i) {
-            while (!entryList[i].empty()) {
-                TlbEntry *entry = entryList[i].front();
-                entryList[i].pop_front();
-                freeList[i].push_back(entry);
-            }
-        }
-    }
-
-    void
-    GpuTLB::setConfigAddress(uint32_t addr)
-    {
-        configAddress = addr;
-    }
-
-    void
-    GpuTLB::invalidateNonGlobal()
-    {
-        DPRINTF(GPUTLB, "Invalidating all non global entries.\n");
-
-        for (int i = 0; i < numSets; ++i) {
-            for (auto entryIt = entryList[i].begin();
-                 entryIt != entryList[i].end();) {
-                if (!(*entryIt)->global) {
-                    freeList[i].push_back(*entryIt);
-                    entryList[i].erase(entryIt++);
-                } else {
-                    ++entryIt;
-                }
-            }
-        }
-    }
-
-    void
-    GpuTLB::demapPage(Addr va, uint64_t asn)
-    {
-
-        int set = (va >> PageShift) & setMask;
-        auto entry = lookupIt(va, false);
-
-        if (entry != entryList[set].end()) {
-            freeList[set].push_back(*entry);
-            entryList[set].erase(entry);
-        }
-    }
-
-
-
-    namespace
-    {
-
-    Cycles
-    localMiscRegAccess(bool read, MiscRegIndex regNum,
-                       ThreadContext *tc, PacketPtr pkt)
-    {
-        if (read) {
-            RegVal data = htole(tc->readMiscReg(regNum));
-            // Make sure we don't trot off the end of data.
-            pkt->setData((uint8_t *)&data);
-        } else {
-            RegVal data = htole(tc->readMiscRegNoEffect(regNum));
-            tc->setMiscReg(regNum, letoh(data));
-        }
-        return Cycles(1);
-    }
-
-    } // anonymous namespace
-
-    Fault
-    GpuTLB::translateInt(bool read, const RequestPtr &req, ThreadContext *tc)
-    {
-        DPRINTF(GPUTLB, "Addresses references internal memory.\n");
-        Addr vaddr = req->getVaddr();
-        Addr prefix = (vaddr >> 3) & IntAddrPrefixMask;
-
-        if (prefix == IntAddrPrefixCPUID) {
-            panic("CPUID memory space not yet implemented!\n");
-        } else if (prefix == IntAddrPrefixMSR) {
-            vaddr = (vaddr >> 3) & ~IntAddrPrefixMask;
-
-            MiscRegIndex regNum;
-            if (!msrAddrToIndex(regNum, vaddr))
-                return std::make_shared<GeneralProtection>(0);
-
-            req->setLocalAccessor(
-                [read,regNum](ThreadContext *tc, PacketPtr pkt)
-                {
-                    return localMiscRegAccess(read, regNum, tc, pkt);
-                }
-            );
-
-            return NoFault;
-        } else if (prefix == IntAddrPrefixIO) {
-            // TODO If CPL > IOPL or in virtual mode, check the I/O permission
-            // bitmap in the TSS.
-
-            Addr IOPort = vaddr & ~IntAddrPrefixMask;
-            // Make sure the address fits in the expected 16 bit IO address
-            // space.
-            assert(!(IOPort & ~0xFFFF));
-            if (IOPort == 0xCF8 && req->getSize() == 4) {
-                req->setLocalAccessor(
-                    [read](ThreadContext *tc, PacketPtr pkt)
-                    {
-                        return localMiscRegAccess(
-                                read, MISCREG_PCI_CONFIG_ADDRESS, tc, pkt);
-                    }
-                );
-            } else if ((IOPort & ~mask(2)) == 0xCFC) {
-                req->setFlags(Request::UNCACHEABLE | Request::STRICT_ORDER);
-                Addr configAddress =
-                    tc->readMiscRegNoEffect(MISCREG_PCI_CONFIG_ADDRESS);
-                if (bits(configAddress, 31, 31)) {
-                    req->setPaddr(PhysAddrPrefixPciConfig |
-                            mbits(configAddress, 30, 2) |
-                            (IOPort & mask(2)));
-                } else {
-                    req->setPaddr(PhysAddrPrefixIO | IOPort);
-                }
-            } else {
-                req->setFlags(Request::UNCACHEABLE | Request::STRICT_ORDER);
-                req->setPaddr(PhysAddrPrefixIO | IOPort);
-            }
-            return NoFault;
-        } else {
-            panic("Access to unrecognized internal address space %#x.\n",
-                  prefix);
-        }
-    }
-
-    /**
-     * TLB_lookup will only perform a TLB lookup returning true on a TLB hit
-     * and false on a TLB miss.
-     * Many of the checks about different modes have been converted to
-     * assertions, since these parts of the code are not really used.
-     * On a hit it will update the LRU stack.
-     */
-    bool
-    GpuTLB::tlbLookup(const RequestPtr &req,
-                      ThreadContext *tc, bool update_stats)
-    {
-        bool tlb_hit = false;
-    #ifndef NDEBUG
-        uint32_t flags = req->getFlags();
-        int seg = flags & SegmentFlagMask;
-    #endif
-
-        assert(seg != SEGMENT_REG_MS);
-        Addr vaddr = req->getVaddr();
-        DPRINTF(GPUTLB, "TLB Lookup for vaddr %#x.\n", vaddr);
-        HandyM5Reg m5Reg = tc->readMiscRegNoEffect(MISCREG_M5_REG);
-
-        if (m5Reg.prot) {
-            DPRINTF(GPUTLB, "In protected mode.\n");
-            // make sure we are in 64-bit mode
-            assert(m5Reg.mode == LongMode);
-
-            // If paging is enabled, do the translation.
-            if (m5Reg.paging) {
-                DPRINTF(GPUTLB, "Paging enabled.\n");
-                //update LRU stack on a hit
-                TlbEntry *entry = lookup(vaddr, true);
-
-                if (entry)
-                    tlb_hit = true;
-
-                if (!update_stats) {
-                    // functional tlb access for memory initialization
-                    // i.e., memory seeding or instr. seeding -> don't update
-                    // TLB and stats
-                    return tlb_hit;
-                }
-
-                stats.localNumTLBAccesses++;
-
-                if (!entry) {
-                    stats.localNumTLBMisses++;
-                } else {
-                    stats.localNumTLBHits++;
-                }
-            }
-        }
-
-        return tlb_hit;
-    }
-
-    Fault
-    GpuTLB::translate(const RequestPtr &req, ThreadContext *tc,
-                      Translation *translation, Mode mode,
-                      bool &delayedResponse, bool timing, int &latency)
-    {
-        uint32_t flags = req->getFlags();
-        int seg = flags & SegmentFlagMask;
-        bool storeCheck = flags & Request::READ_MODIFY_WRITE;
-
-        // If this is true, we're dealing with a request
-        // to a non-memory address space.
-        if (seg == SEGMENT_REG_MS) {
-            return translateInt(mode == Mode::Read, req, tc);
-        }
-
-        delayedResponse = false;
-        Addr vaddr = req->getVaddr();
-        DPRINTF(GPUTLB, "Translating vaddr %#x.\n", vaddr);
-
-        HandyM5Reg m5Reg = tc->readMiscRegNoEffect(MISCREG_M5_REG);
-
-        // If protected mode has been enabled...
-        if (m5Reg.prot) {
-            DPRINTF(GPUTLB, "In protected mode.\n");
-            // If we're not in 64-bit mode, do protection/limit checks
-            if (m5Reg.mode != LongMode) {
-                DPRINTF(GPUTLB, "Not in long mode. Checking segment "
-                        "protection.\n");
-
-                // Check for a null segment selector.
-                if (!(seg == SEGMENT_REG_TSG || seg == SYS_SEGMENT_REG_IDTR ||
-                    seg == SEGMENT_REG_HS || seg == SEGMENT_REG_LS)
-                    && !tc->readMiscRegNoEffect(MISCREG_SEG_SEL(seg))) {
-                    return std::make_shared<GeneralProtection>(0);
-                }
-
-                bool expandDown = false;
-                SegAttr attr = tc->readMiscRegNoEffect(MISCREG_SEG_ATTR(seg));
-
-                if (seg >= SEGMENT_REG_ES && seg <= SEGMENT_REG_HS) {
-                    if (!attr.writable && (mode == BaseMMU::Write ||
-                        storeCheck))
-                        return std::make_shared<GeneralProtection>(0);
-
-                    if (!attr.readable && mode == BaseMMU::Read)
-                        return std::make_shared<GeneralProtection>(0);
-
-                    expandDown = attr.expandDown;
-
-                }
-
-                Addr base = tc->readMiscRegNoEffect(MISCREG_SEG_BASE(seg));
-                Addr limit = tc->readMiscRegNoEffect(MISCREG_SEG_LIMIT(seg));
-                // This assumes we're not in 64 bit mode. If we were, the
-                // default address size is 64 bits, overridable to 32.
-                int size = 32;
-                bool sizeOverride = (flags & (AddrSizeFlagBit << FlagShift));
-                SegAttr csAttr = tc->readMiscRegNoEffect(MISCREG_CS_ATTR);
-
-                if ((csAttr.defaultSize && sizeOverride) ||
-                    (!csAttr.defaultSize && !sizeOverride)) {
-                    size = 16;
-                }
-
-                Addr offset = bits(vaddr - base, size - 1, 0);
-                Addr endOffset = offset + req->getSize() - 1;
-
-                if (expandDown) {
-                    DPRINTF(GPUTLB, "Checking an expand down segment.\n");
-                    warn_once("Expand down segments are untested.\n");
-
-                    if (offset <= limit || endOffset <= limit)
-                        return std::make_shared<GeneralProtection>(0);
-                } else {
-                    if (offset > limit || endOffset > limit)
-                        return std::make_shared<GeneralProtection>(0);
-                }
-            }
-
-            // If paging is enabled, do the translation.
-            if (m5Reg.paging) {
-                DPRINTF(GPUTLB, "Paging enabled.\n");
-                // The vaddr already has the segment base applied.
-                TlbEntry *entry = lookup(vaddr);
-                stats.localNumTLBAccesses++;
-
-                if (!entry) {
-                    stats.localNumTLBMisses++;
-                    if (timing) {
-                        latency = missLatency1;
-                    }
-
-                    if (FullSystem) {
-                        fatal("GpuTLB doesn't support full-system mode\n");
-                    } else {
-                        DPRINTF(GPUTLB, "Handling a TLB miss for address %#x "
-                                "at pc %#x.\n", vaddr,
-                                tc->pcState().instAddr());
-
-                        Process *p = tc->getProcessPtr();
-                        const EmulationPageTable::Entry *pte =
-                            p->pTable->lookup(vaddr);
-
-                        if (!pte && mode != BaseMMU::Execute) {
-                            // penalize a "page fault" more
-                            if (timing)
-                                latency += missLatency2;
-
-                            if (p->fixupFault(vaddr))
-                                pte = p->pTable->lookup(vaddr);
-                        }
-
-                        if (!pte) {
-                            return std::make_shared<PageFault>(vaddr, true,
-                                                               mode, true,
-                                                               false);
-                        } else {
-                            Addr alignedVaddr = p->pTable->pageAlign(vaddr);
-
-                            DPRINTF(GPUTLB, "Mapping %#x to %#x\n",
-                                    alignedVaddr, pte->paddr);
-
-                            TlbEntry gpuEntry(p->pid(), alignedVaddr,
-                                              pte->paddr, false, false);
-                            entry = insert(alignedVaddr, gpuEntry);
-                        }
-
-                        DPRINTF(GPUTLB, "Miss was serviced.\n");
-                    }
-                } else {
-                    stats.localNumTLBHits++;
-
-                    if (timing) {
-                        latency = hitLatency;
-                    }
-                }
-
-                // Do paging protection checks.
-                bool inUser = (m5Reg.cpl == 3 &&
-                               !(flags & (CPL0FlagBit << FlagShift)));
-
-                CR0 cr0 = tc->readMiscRegNoEffect(MISCREG_CR0);
-                bool badWrite = (!entry->writable && (inUser || cr0.wp));
-
-                if ((inUser && !entry->user) || (mode == BaseMMU::Write &&
-                     badWrite)) {
-                    // The page must have been present to get into the TLB in
-                    // the first place. We'll assume the reserved bits are
-                    // fine even though we're not checking them.
-                    return std::make_shared<PageFault>(vaddr, true, mode,
-                                                       inUser, false);
-                }
-
-                if (storeCheck && badWrite) {
-                    // This would fault if this were a write, so return a page
-                    // fault that reflects that happening.
-                    return std::make_shared<PageFault>(vaddr, true,
-                                                       BaseMMU::Write,
-                                                       inUser, false);
-                }
-
-
-                DPRINTF(GPUTLB, "Entry found with paddr %#x, doing protection "
-                        "checks.\n", entry->paddr);
-
-                int page_size = entry->size();
-                Addr paddr = entry->paddr | (vaddr & (page_size - 1));
-                DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, paddr);
-                req->setPaddr(paddr);
-
-                if (entry->uncacheable)
-                    req->setFlags(Request::UNCACHEABLE);
-            } else {
-                //Use the address which already has segmentation applied.
-                DPRINTF(GPUTLB, "Paging disabled.\n");
-                DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, vaddr);
-                req->setPaddr(vaddr);
-            }
-        } else {
-            // Real mode
-            DPRINTF(GPUTLB, "In real mode.\n");
-            DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, vaddr);
-            req->setPaddr(vaddr);
-        }
-
-        // Check for an access to the local APIC
-        if (FullSystem) {
-            LocalApicBase localApicBase =
-                tc->readMiscRegNoEffect(MISCREG_APIC_BASE);
-
-            Addr baseAddr = localApicBase.base * PageBytes;
-            Addr paddr = req->getPaddr();
-
-            if (baseAddr <= paddr && baseAddr + PageBytes > paddr) {
-                // Force the access to be uncacheable.
-                req->setFlags(Request::UNCACHEABLE);
-                req->setPaddr(x86LocalAPICAddress(tc->contextId(),
-                                                  paddr - baseAddr));
-            }
-        }
-
-        return NoFault;
-    };
-
-    Fault
-    GpuTLB::translateAtomic(const RequestPtr &req, ThreadContext *tc,
-                            Mode mode, int &latency)
-    {
-        bool delayedResponse;
-
-        return GpuTLB::translate(req, tc, nullptr, mode, delayedResponse,
-            false, latency);
-    }
-
-    void
-    GpuTLB::translateTiming(const RequestPtr &req, ThreadContext *tc,
-            Translation *translation, Mode mode, int &latency)
-    {
-        bool delayedResponse;
-        assert(translation);
-
-        Fault fault = GpuTLB::translate(req, tc, translation, mode,
-                                        delayedResponse, true, latency);
-
-        if (!delayedResponse)
-            translation->finish(fault, req, tc, mode);
-    }
-
-    Walker*
-    GpuTLB::getWalker()
-    {
-        return walker;
-    }
-
-
-    void
-    GpuTLB::serialize(CheckpointOut &cp) const
-    {
-    }
-
-    void
-    GpuTLB::unserialize(CheckpointIn &cp)
-    {
-    }
-
-    /**
-     * Do the TLB lookup for this coalesced request and schedule
-     * another event <TLB access latency> cycles later.
-     */
-
-    void
-    GpuTLB::issueTLBLookup(PacketPtr pkt)
-    {
-        assert(pkt);
-        assert(pkt->senderState);
-
-        Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
-                                        X86ISA::PageBytes);
-
-        TranslationState *sender_state =
-                safe_cast<TranslationState*>(pkt->senderState);
-
-        bool update_stats = !sender_state->isPrefetch;
-        ThreadContext * tmp_tc = sender_state->tc;
-
-        DPRINTF(GPUTLB, "Translation req. for virt. page addr %#x\n",
-                virt_page_addr);
-
-        int req_cnt = sender_state->reqCnt.back();
-
-        if (update_stats) {
-            stats.accessCycles -= (curTick() * req_cnt);
-            stats.localCycles -= curTick();
-            updatePageFootprint(virt_page_addr);
-            stats.globalNumTLBAccesses += req_cnt;
-        }
-
-        tlbOutcome lookup_outcome = TLB_MISS;
-        const RequestPtr &tmp_req = pkt->req;
-
-        // Access the TLB and figure out if it's a hit or a miss.
-        bool success = tlbLookup(tmp_req, tmp_tc, update_stats);
-
-        if (success) {
-            lookup_outcome = TLB_HIT;
-            // Put the entry in SenderState
-            TlbEntry *entry = lookup(tmp_req->getVaddr(), false);
-            assert(entry);
-
-            auto p = sender_state->tc->getProcessPtr();
-            sender_state->tlbEntry =
-                new TlbEntry(p->pid(), entry->vaddr, entry->paddr,
-                             false, false);
-
-            if (update_stats) {
-                // the reqCnt has an entry per level, so its size tells us
-                // which level we are in
-                sender_state->hitLevel = sender_state->reqCnt.size();
-                stats.globalNumTLBHits += req_cnt;
-            }
-        } else {
-            if (update_stats)
-                stats.globalNumTLBMisses += req_cnt;
-        }
-
-        /*
-         * We now know the TLB lookup outcome (if it's a hit or a miss), as
-         * well as the TLB access latency.
-         *
-         * We create and schedule a new TLBEvent which will help us take the
-         * appropriate actions (e.g., update TLB on a hit, send request to
-         * lower level TLB on a miss, or start a page walk if this was the
-         * last-level TLB)
-         */
-        TLBEvent *tlb_event =
-            new TLBEvent(this, virt_page_addr, lookup_outcome, pkt);
-
-        if (translationReturnEvent.count(virt_page_addr)) {
-            panic("Virtual Page Address %#x already has a return event\n",
-                  virt_page_addr);
-        }
-
-        translationReturnEvent[virt_page_addr] = tlb_event;
-        assert(tlb_event);
-
-        DPRINTF(GPUTLB, "schedule translationReturnEvent @ curTick %d\n",
-                curTick() + cyclesToTicks(Cycles(hitLatency)));
-
-        schedule(tlb_event, curTick() + cyclesToTicks(Cycles(hitLatency)));
-    }
-
-    GpuTLB::TLBEvent::TLBEvent(GpuTLB* _tlb, Addr _addr,
-        tlbOutcome tlb_outcome, PacketPtr _pkt)
-            : Event(CPU_Tick_Pri), tlb(_tlb), virtPageAddr(_addr),
-              outcome(tlb_outcome), pkt(_pkt)
-    {
-    }
-
-    /**
-     * Do Paging protection checks. If we encounter a page fault, then
-     * an assertion is fired.
-     */
-    void
-    GpuTLB::pagingProtectionChecks(ThreadContext *tc, PacketPtr pkt,
-            TlbEntry * tlb_entry, Mode mode)
-    {
-        HandyM5Reg m5Reg = tc->readMiscRegNoEffect(MISCREG_M5_REG);
-        uint32_t flags = pkt->req->getFlags();
-        bool storeCheck = flags & Request::READ_MODIFY_WRITE;
-
-        // Do paging protection checks.
-        bool inUser
-            = (m5Reg.cpl == 3 && !(flags & (CPL0FlagBit << FlagShift)));
-        CR0 cr0 = tc->readMiscRegNoEffect(MISCREG_CR0);
-
-        bool badWrite = (!tlb_entry->writable && (inUser || cr0.wp));
-
-        if ((inUser && !tlb_entry->user) ||
-            (mode == BaseMMU::Write && badWrite)) {
-            // The page must have been present to get into the TLB in
-            // the first place. We'll assume the reserved bits are
-            // fine even though we're not checking them.
-            panic("Page fault detected");
-        }
-
-        if (storeCheck && badWrite) {
-            // This would fault if this were a write, so return a page
-            // fault that reflects that happening.
-            panic("Page fault detected");
-        }
-    }
-
-    /**
-     * handleTranslationReturn is called on a TLB hit,
-     * when a TLB miss returns or when a page fault returns.
-     * The latter calls handelHit with TLB miss as tlbOutcome.
-     */
-    void
-    GpuTLB::handleTranslationReturn(Addr virt_page_addr,
-        tlbOutcome tlb_outcome, PacketPtr pkt)
-    {
-        assert(pkt);
-        Addr vaddr = pkt->req->getVaddr();
-
-        TranslationState *sender_state =
-            safe_cast<TranslationState*>(pkt->senderState);
-
-        ThreadContext *tc = sender_state->tc;
-        Mode mode = sender_state->tlbMode;
-
-        TlbEntry *local_entry, *new_entry;
-
-        if (tlb_outcome == TLB_HIT) {
-            DPRINTF(GPUTLB, "Translation Done - TLB Hit for addr %#x\n",
-                vaddr);
-            local_entry = sender_state->tlbEntry;
-        } else {
-            DPRINTF(GPUTLB, "Translation Done - TLB Miss for addr %#x\n",
-                    vaddr);
-
-            /**
-             * We are returning either from a page walk or from a hit at a
-             * lower TLB level. The senderState should be "carrying" a pointer
-             * to the correct TLBEntry.
-             */
-            new_entry = sender_state->tlbEntry;
-            assert(new_entry);
-            local_entry = new_entry;
-
-            if (allocationPolicy) {
-                DPRINTF(GPUTLB, "allocating entry w/ addr %#x\n",
-                        virt_page_addr);
-
-                local_entry = insert(virt_page_addr, *new_entry);
-            }
-
-            assert(local_entry);
-        }
-
-        /**
-         * At this point the packet carries an up-to-date tlbEntry pointer
-         * in its senderState.
-         * Next step is to do the paging protection checks.
-         */
-        DPRINTF(GPUTLB, "Entry found with vaddr %#x,  doing protection checks "
-                "while paddr was %#x.\n", local_entry->vaddr,
-                local_entry->paddr);
-
-        pagingProtectionChecks(tc, pkt, local_entry, mode);
-        int page_size = local_entry->size();
-        Addr paddr = local_entry->paddr | (vaddr & (page_size - 1));
-        DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, paddr);
-
-        // Since this packet will be sent through the cpu side port,
-        // it must be converted to a response pkt if it is not one already
-        if (pkt->isRequest()) {
-            pkt->makeTimingResponse();
-        }
-
-        pkt->req->setPaddr(paddr);
-
-        if (local_entry->uncacheable) {
-             pkt->req->setFlags(Request::UNCACHEABLE);
-        }
-
-        //send packet back to coalescer
-        cpuSidePort[0]->sendTimingResp(pkt);
-        //schedule cleanup event
-        cleanupQueue.push(virt_page_addr);
-
-        // schedule this only once per cycle.
-        // The check is required because we might have multiple translations
-        // returning the same cycle
-        // this is a maximum priority event and must be on the same cycle
-        // as the cleanup event in TLBCoalescer to avoid a race with
-        // IssueProbeEvent caused by TLBCoalescer::MemSidePort::recvReqRetry
-        if (!cleanupEvent.scheduled())
-            schedule(cleanupEvent, curTick());
-    }
-
-    /**
-     * Here we take the appropriate actions based on the result of the
-     * TLB lookup.
-     */
-    void
-    GpuTLB::translationReturn(Addr virtPageAddr, tlbOutcome outcome,
-                              PacketPtr pkt)
-    {
-        DPRINTF(GPUTLB, "Triggered TLBEvent for addr %#x\n", virtPageAddr);
-
-        assert(translationReturnEvent[virtPageAddr]);
-        assert(pkt);
-
-        TranslationState *tmp_sender_state =
-            safe_cast<TranslationState*>(pkt->senderState);
-
-        int req_cnt = tmp_sender_state->reqCnt.back();
-        bool update_stats = !tmp_sender_state->isPrefetch;
-
-
-        if (outcome == TLB_HIT) {
-            handleTranslationReturn(virtPageAddr, TLB_HIT, pkt);
-
-            if (update_stats) {
-                stats.accessCycles += (req_cnt * curTick());
-                stats.localCycles += curTick();
-            }
-
-        } else if (outcome == TLB_MISS) {
-
-            DPRINTF(GPUTLB, "This is a TLB miss\n");
-            if (update_stats) {
-                stats.accessCycles += (req_cnt*curTick());
-                stats.localCycles += curTick();
-            }
-
-            if (hasMemSidePort) {
-                // the one cyle added here represent the delay from when we get
-                // the reply back till when we propagate it to the coalescer
-                // above.
-                if (update_stats) {
-                    stats.accessCycles += (req_cnt * 1);
-                    stats.localCycles += 1;
-                }
-
-                /**
-                 * There is a TLB below. Send the coalesced request.
-                 * We actually send the very first packet of all the
-                 * pending packets for this virtual page address.
-                 */
-                if (!memSidePort[0]->sendTimingReq(pkt)) {
-                    DPRINTF(GPUTLB, "Failed sending translation request to "
-                            "lower level TLB for addr %#x\n", virtPageAddr);
-
-                    memSidePort[0]->retries.push_back(pkt);
-                } else {
-                    DPRINTF(GPUTLB, "Sent translation request to lower level "
-                            "TLB for addr %#x\n", virtPageAddr);
-                }
-            } else {
-                //this is the last level TLB. Start a page walk
-                DPRINTF(GPUTLB, "Last level TLB - start a page walk for "
-                        "addr %#x\n", virtPageAddr);
-
-                if (update_stats)
-                    stats.pageTableCycles -= (req_cnt*curTick());
-
-                TLBEvent *tlb_event = translationReturnEvent[virtPageAddr];
-                assert(tlb_event);
-                tlb_event->updateOutcome(PAGE_WALK);
-                schedule(tlb_event,
-                         curTick() + cyclesToTicks(Cycles(missLatency2)));
-            }
-        } else if (outcome == PAGE_WALK) {
-            if (update_stats)
-                stats.pageTableCycles += (req_cnt*curTick());
-
-            // Need to access the page table and update the TLB
-            DPRINTF(GPUTLB, "Doing a page walk for address %#x\n",
-                    virtPageAddr);
-
-            TranslationState *sender_state =
-                safe_cast<TranslationState*>(pkt->senderState);
-
-            Process *p = sender_state->tc->getProcessPtr();
-            Addr vaddr = pkt->req->getVaddr();
-
-            Addr alignedVaddr = p->pTable->pageAlign(vaddr);
-            assert(alignedVaddr == virtPageAddr);
-
-            const EmulationPageTable::Entry *pte = p->pTable->lookup(vaddr);
-            if (!pte && sender_state->tlbMode != BaseMMU::Execute &&
-                    p->fixupFault(vaddr)) {
-                pte = p->pTable->lookup(vaddr);
-            }
-
-            if (pte) {
-                DPRINTF(GPUTLB, "Mapping %#x to %#x\n", alignedVaddr,
-                        pte->paddr);
-
-                sender_state->tlbEntry =
-                    new TlbEntry(p->pid(), virtPageAddr, pte->paddr, false,
-                                 false);
-            } else {
-                sender_state->tlbEntry = nullptr;
-            }
-
-            handleTranslationReturn(virtPageAddr, TLB_MISS, pkt);
-        } else if (outcome == MISS_RETURN) {
-            /** we add an extra cycle in the return path of the translation
-             * requests in between the various TLB levels.
-             */
-            handleTranslationReturn(virtPageAddr, TLB_MISS, pkt);
-        } else {
-            panic("Unexpected TLB outcome %d", outcome);
-        }
-    }
-
-    void
-    GpuTLB::TLBEvent::process()
-    {
-        tlb->translationReturn(virtPageAddr, outcome, pkt);
-    }
-
-    const char*
-    GpuTLB::TLBEvent::description() const
-    {
-        return "trigger translationDoneEvent";
-    }
-
-    void
-    GpuTLB::TLBEvent::updateOutcome(tlbOutcome _outcome)
-    {
-        outcome = _outcome;
-    }
-
-    Addr
-    GpuTLB::TLBEvent::getTLBEventVaddr()
-    {
-        return virtPageAddr;
-    }
-
-    /**
-     * recvTiming receives a coalesced timing request from a TLBCoalescer
-     * and it calls issueTLBLookup()
-     * It only rejects the packet if we have exceeded the max
-     * outstanding number of requests for the TLB
-     */
-    bool
-    GpuTLB::CpuSidePort::recvTimingReq(PacketPtr pkt)
-    {
-        if (tlb->outstandingReqs < tlb->maxCoalescedReqs) {
-            tlb->issueTLBLookup(pkt);
-            // update number of outstanding translation requests
-            tlb->outstandingReqs++;
-            return true;
-         } else {
-            DPRINTF(GPUTLB, "Reached maxCoalescedReqs number %d\n",
-                    tlb->outstandingReqs);
-            return false;
-         }
-    }
-
-    /**
-     * handleFuncTranslationReturn is called on a TLB hit,
-     * when a TLB miss returns or when a page fault returns.
-     * It updates LRU, inserts the TLB entry on a miss
-     * depending on the allocation policy and does the required
-     * protection checks. It does NOT create a new packet to
-     * update the packet's addr; this is done in hsail-gpu code.
-     */
-    void
-    GpuTLB::handleFuncTranslationReturn(PacketPtr pkt, tlbOutcome tlb_outcome)
-    {
-        TranslationState *sender_state =
-            safe_cast<TranslationState*>(pkt->senderState);
-
-        ThreadContext *tc = sender_state->tc;
-        Mode mode = sender_state->tlbMode;
-        Addr vaddr = pkt->req->getVaddr();
-
-        TlbEntry *local_entry, *new_entry;
-
-        if (tlb_outcome == TLB_HIT) {
-            DPRINTF(GPUTLB, "Functional Translation Done - TLB hit for addr "
-                    "%#x\n", vaddr);
-
-            local_entry = sender_state->tlbEntry;
-        } else {
-            DPRINTF(GPUTLB, "Functional Translation Done - TLB miss for addr "
-                    "%#x\n", vaddr);
-
-            /**
-             * We are returning either from a page walk or from a hit at a
-             * lower TLB level. The senderState should be "carrying" a pointer
-             * to the correct TLBEntry.
-             */
-            new_entry = sender_state->tlbEntry;
-            assert(new_entry);
-            local_entry = new_entry;
-
-            if (allocationPolicy) {
-                Addr virt_page_addr = roundDown(vaddr, X86ISA::PageBytes);
-
-                DPRINTF(GPUTLB, "allocating entry w/ addr %#x\n",
-                        virt_page_addr);
-
-                local_entry = insert(virt_page_addr, *new_entry);
-            }
-
-            assert(local_entry);
-        }
-
-        DPRINTF(GPUTLB, "Entry found with vaddr %#x, doing protection checks "
-                "while paddr was %#x.\n", local_entry->vaddr,
-                local_entry->paddr);
-
-        /**
-         * Do paging checks if it's a normal functional access.  If it's for a
-         * prefetch, then sometimes you can try to prefetch something that
-         * won't pass protection. We don't actually want to fault becuase there
-         * is no demand access to deem this a violation.  Just put it in the
-         * TLB and it will fault if indeed a future demand access touches it in
-         * violation.
-         *
-         * This feature could be used to explore security issues around
-         * speculative memory accesses.
-         */
-        if (!sender_state->isPrefetch && sender_state->tlbEntry)
-            pagingProtectionChecks(tc, pkt, local_entry, mode);
-
-        int page_size = local_entry->size();
-        Addr paddr = local_entry->paddr | (vaddr & (page_size - 1));
-        DPRINTF(GPUTLB, "Translated %#x -> %#x.\n", vaddr, paddr);
-
-        pkt->req->setPaddr(paddr);
-
-        if (local_entry->uncacheable)
-             pkt->req->setFlags(Request::UNCACHEABLE);
-    }
-
-    // This is used for atomic translations. Need to
-    // make it all happen during the same cycle.
-    void
-    GpuTLB::CpuSidePort::recvFunctional(PacketPtr pkt)
-    {
-        TranslationState *sender_state =
-            safe_cast<TranslationState*>(pkt->senderState);
-
-        ThreadContext *tc = sender_state->tc;
-        bool update_stats = !sender_state->isPrefetch;
-
-        Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
-                                        X86ISA::PageBytes);
-
-        if (update_stats)
-            tlb->updatePageFootprint(virt_page_addr);
-
-        // do the TLB lookup without updating the stats
-        bool success = tlb->tlbLookup(pkt->req, tc, update_stats);
-        tlbOutcome tlb_outcome = success ? TLB_HIT : TLB_MISS;
-
-        // functional mode means no coalescing
-        // global metrics are the same as the local metrics
-        if (update_stats) {
-            tlb->stats.globalNumTLBAccesses++;
-
-            if (success) {
-                sender_state->hitLevel = sender_state->reqCnt.size();
-                tlb->stats.globalNumTLBHits++;
-            }
-        }
-
-        if (!success) {
-            if (update_stats)
-                tlb->stats.globalNumTLBMisses++;
-            if (tlb->hasMemSidePort) {
-                // there is a TLB below -> propagate down the TLB hierarchy
-                tlb->memSidePort[0]->sendFunctional(pkt);
-                // If no valid translation from a prefetch, then just return
-                if (sender_state->isPrefetch && !pkt->req->hasPaddr())
-                    return;
-            } else {
-                // Need to access the page table and update the TLB
-                DPRINTF(GPUTLB, "Doing a page walk for address %#x\n",
-                        virt_page_addr);
-
-                Process *p = tc->getProcessPtr();
-
-                Addr vaddr = pkt->req->getVaddr();
-
-                Addr alignedVaddr = p->pTable->pageAlign(vaddr);
-                assert(alignedVaddr == virt_page_addr);
-
-                const EmulationPageTable::Entry *pte =
-                        p->pTable->lookup(vaddr);
-                if (!pte && sender_state->tlbMode != BaseMMU::Execute &&
-                        p->fixupFault(vaddr)) {
-                    pte = p->pTable->lookup(vaddr);
-                }
-
-                if (!sender_state->isPrefetch) {
-                    // no PageFaults are permitted after
-                    // the second page table lookup
-                    assert(pte);
-
-                    DPRINTF(GPUTLB, "Mapping %#x to %#x\n", alignedVaddr,
-                            pte->paddr);
-
-                    sender_state->tlbEntry =
-                        new TlbEntry(p->pid(), virt_page_addr,
-                                     pte->paddr, false, false);
-                } else {
-                    // If this was a prefetch, then do the normal thing if it
-                    // was a successful translation.  Otherwise, send an empty
-                    // TLB entry back so that it can be figured out as empty
-                    // and handled accordingly.
-                    if (pte) {
-                        DPRINTF(GPUTLB, "Mapping %#x to %#x\n", alignedVaddr,
-                                pte->paddr);
-
-                        sender_state->tlbEntry =
-                            new TlbEntry(p->pid(), virt_page_addr,
-                                         pte->paddr, false, false);
-                    } else {
-                        DPRINTF(GPUPrefetch, "Prefetch failed %#x\n",
-                                alignedVaddr);
-
-                        sender_state->tlbEntry = nullptr;
-
-                        return;
-                    }
-                }
-            }
-        } else {
-            DPRINTF(GPUPrefetch, "Functional Hit for vaddr %#x\n",
-                    tlb->lookup(pkt->req->getVaddr()));
-
-            TlbEntry *entry = tlb->lookup(pkt->req->getVaddr(),
-                                             update_stats);
-
-            assert(entry);
-
-            auto p = sender_state->tc->getProcessPtr();
-            sender_state->tlbEntry =
-                new TlbEntry(p->pid(), entry->vaddr, entry->paddr,
-                             false, false);
-        }
-        // This is the function that would populate pkt->req with the paddr of
-        // the translation. But if no translation happens (i.e Prefetch fails)
-        // then the early returns in the above code wiill keep this function
-        // from executing.
-        tlb->handleFuncTranslationReturn(pkt, tlb_outcome);
-    }
-
-    void
-    GpuTLB::CpuSidePort::recvReqRetry()
-    {
-        // The CPUSidePort never sends anything but replies. No retries
-        // expected.
-        panic("recvReqRetry called");
-    }
-
-    AddrRangeList
-    GpuTLB::CpuSidePort::getAddrRanges() const
-    {
-        // currently not checked by the requestor
-        AddrRangeList ranges;
-
-        return ranges;
-    }
-
-    /**
-     * MemSidePort receives the packet back.
-     * We need to call the handleTranslationReturn
-     * and propagate up the hierarchy.
-     */
-    bool
-    GpuTLB::MemSidePort::recvTimingResp(PacketPtr pkt)
-    {
-        Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
-                                        X86ISA::PageBytes);
-
-        DPRINTF(GPUTLB, "MemSidePort recvTiming for virt_page_addr %#x\n",
-                virt_page_addr);
-
-        TLBEvent *tlb_event = tlb->translationReturnEvent[virt_page_addr];
-        assert(tlb_event);
-        assert(virt_page_addr == tlb_event->getTLBEventVaddr());
-
-        tlb_event->updateOutcome(MISS_RETURN);
-        tlb->schedule(tlb_event, curTick()+tlb->clockPeriod());
-
-        return true;
-    }
-
-    void
-    GpuTLB::MemSidePort::recvReqRetry()
-    {
-        // No retries should reach the TLB. The retries
-        // should only reach the TLBCoalescer.
-        panic("recvReqRetry called");
-    }
-
-    void
-    GpuTLB::cleanup()
-    {
-        while (!cleanupQueue.empty()) {
-            Addr cleanup_addr = cleanupQueue.front();
-            cleanupQueue.pop();
-
-            // delete TLBEvent
-            TLBEvent * old_tlb_event = translationReturnEvent[cleanup_addr];
-            delete old_tlb_event;
-            translationReturnEvent.erase(cleanup_addr);
-
-            // update number of outstanding requests
-            outstandingReqs--;
-        }
-
-        /** the higher level coalescer should retry if it has
-         * any pending requests.
-         */
-        for (int i = 0; i < cpuSidePort.size(); ++i) {
-            cpuSidePort[i]->sendRetryReq();
-        }
-    }
-
-    void
-    GpuTLB::updatePageFootprint(Addr virt_page_addr)
-    {
-
-        std::pair<AccessPatternTable::iterator, bool> ret;
-
-        AccessInfo tmp_access_info;
-        tmp_access_info.lastTimeAccessed = 0;
-        tmp_access_info.accessesPerPage = 0;
-        tmp_access_info.totalReuseDistance = 0;
-        tmp_access_info.sumDistance = 0;
-        tmp_access_info.meanDistance = 0;
-
-        ret = TLBFootprint.insert(
-            AccessPatternTable::value_type(virt_page_addr, tmp_access_info));
-
-        bool first_page_access = ret.second;
-
-        if (first_page_access) {
-            stats.numUniquePages++;
-        } else  {
-            int accessed_before;
-            accessed_before  = curTick() - ret.first->second.lastTimeAccessed;
-            ret.first->second.totalReuseDistance += accessed_before;
-        }
-
-        ret.first->second.accessesPerPage++;
-        ret.first->second.lastTimeAccessed = curTick();
-
-        if (accessDistance) {
-            ret.first->second.localTLBAccesses
-                .push_back(stats.localNumTLBAccesses.value());
-        }
-    }
-
-    void
-    GpuTLB::exitCallback()
-    {
-        std::ostream *page_stat_file = nullptr;
-
-        if (accessDistance) {
-
-            // print per page statistics to a separate file (.csv format)
-            // simout is the gem5 output directory (default is m5out or the one
-            // specified with -d
-            page_stat_file = simout.create(name().c_str())->stream();
-
-            // print header
-            *page_stat_file
-                << "page,max_access_distance,mean_access_distance, "
-                << "stddev_distance" << std::endl;
-        }
-
-        // update avg. reuse distance footprint
-        unsigned int sum_avg_reuse_distance_per_page = 0;
-
-        // iterate through all pages seen by this TLB
-        for (auto &iter : TLBFootprint) {
-            sum_avg_reuse_distance_per_page += iter.second.totalReuseDistance /
-                                               iter.second.accessesPerPage;
-
-            if (accessDistance) {
-                unsigned int tmp = iter.second.localTLBAccesses[0];
-                unsigned int prev = tmp;
-
-                for (int i = 0; i < iter.second.localTLBAccesses.size(); ++i) {
-                    if (i) {
-                        tmp = prev + 1;
-                    }
-
-                    prev = iter.second.localTLBAccesses[i];
-                    // update the localTLBAccesses value
-                    // with the actual differece
-                    iter.second.localTLBAccesses[i] -= tmp;
-                    // compute the sum of AccessDistance per page
-                    // used later for mean
-                    iter.second.sumDistance +=
-                        iter.second.localTLBAccesses[i];
-                }
-
-                iter.second.meanDistance =
-                    iter.second.sumDistance / iter.second.accessesPerPage;
-
-                // compute std_dev and max  (we need a second round because we
-                // need to know the mean value
-                unsigned int max_distance = 0;
-                unsigned int stddev_distance = 0;
-
-                for (int i = 0; i < iter.second.localTLBAccesses.size(); ++i) {
-                    unsigned int tmp_access_distance =
-                        iter.second.localTLBAccesses[i];
-
-                    if (tmp_access_distance > max_distance) {
-                        max_distance = tmp_access_distance;
-                    }
-
-                    unsigned int diff =
-                        tmp_access_distance - iter.second.meanDistance;
-                    stddev_distance += pow(diff, 2);
-
-                }
-
-                stddev_distance =
-                    sqrt(stddev_distance/iter.second.accessesPerPage);
-
-                if (page_stat_file) {
-                    *page_stat_file << std::hex << iter.first << ",";
-                    *page_stat_file << std::dec << max_distance << ",";
-                    *page_stat_file << std::dec << iter.second.meanDistance
-                                    << ",";
-                    *page_stat_file << std::dec << stddev_distance;
-                    *page_stat_file << std::endl;
-                }
-
-                // erase the localTLBAccesses array
-                iter.second.localTLBAccesses.clear();
-            }
-        }
-
-        if (!TLBFootprint.empty()) {
-            stats.avgReuseDistance =
-                sum_avg_reuse_distance_per_page / TLBFootprint.size();
-        }
-
-        //clear the TLBFootprint map
-        TLBFootprint.clear();
-    }
-
-    GpuTLB::GpuTLBStats::GpuTLBStats(statistics::Group *parent)
-        : statistics::Group(parent),
-          ADD_STAT(localNumTLBAccesses, "Number of TLB accesses"),
-          ADD_STAT(localNumTLBHits, "Number of TLB hits"),
-          ADD_STAT(localNumTLBMisses, "Number of TLB misses"),
-          ADD_STAT(localTLBMissRate, "TLB miss rate"),
-          ADD_STAT(globalNumTLBAccesses, "Number of TLB accesses"),
-          ADD_STAT(globalNumTLBHits, "Number of TLB hits"),
-          ADD_STAT(globalNumTLBMisses, "Number of TLB misses"),
-          ADD_STAT(globalTLBMissRate, "TLB miss rate"),
-          ADD_STAT(accessCycles, "Cycles spent accessing this TLB level"),
-          ADD_STAT(pageTableCycles, "Cycles spent accessing the page table"),
-          ADD_STAT(numUniquePages, "Number of unique pages touched"),
-          ADD_STAT(localCycles, "Number of cycles spent in queue for all "
-                   "incoming reqs"),
-          ADD_STAT(localLatency, "Avg. latency over incoming coalesced reqs"),
-          ADD_STAT(avgReuseDistance, "avg. reuse distance over all pages (in "
-                   "ticks)")
-    {
-        localLatency = localCycles / localNumTLBAccesses;
-
-        localTLBMissRate = 100 * localNumTLBMisses / localNumTLBAccesses;
-        globalTLBMissRate = 100 * globalNumTLBMisses / globalNumTLBAccesses;
-    }
-} // namespace X86ISA
-} // namespace gem5
diff --git a/src/arch/amdgpu/gcn3/tlb.hh b/src/arch/amdgpu/gcn3/tlb.hh
deleted file mode 100644
index 944c0ac..0000000
--- a/src/arch/amdgpu/gcn3/tlb.hh
+++ /dev/null
@@ -1,445 +0,0 @@
-/*
- * Copyright (c) 2011-2015 Advanced Micro Devices, Inc.
- * All rights reserved.
- *
- * For use for simulation and test purposes only
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright notice,
- * this list of conditions and the following disclaimer.
- *
- * 2. Redistributions in binary form must reproduce the above copyright notice,
- * this list of conditions and the following disclaimer in the documentation
- * and/or other materials provided with the distribution.
- *
- * 3. Neither the name of the copyright holder nor the names of its
- * contributors may be used to endorse or promote products derived from this
- * software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef __GPU_TLB_HH__
-#define __GPU_TLB_HH__
-
-#include <fstream>
-#include <list>
-#include <queue>
-#include <string>
-#include <vector>
-
-#include "arch/generic/tlb.hh"
-#include "arch/x86/pagetable.hh"
-#include "arch/x86/pagetable_walker.hh"
-#include "arch/x86/regs/segment.hh"
-#include "base/callback.hh"
-#include "base/logging.hh"
-#include "base/statistics.hh"
-#include "base/stats/group.hh"
-#include "gpu-compute/compute_unit.hh"
-#include "mem/port.hh"
-#include "mem/request.hh"
-#include "params/X86GPUTLB.hh"
-#include "sim/clocked_object.hh"
-#include "sim/sim_object.hh"
-
-namespace gem5
-{
-
-class BaseTLB;
-class Packet;
-class ThreadContext;
-
-namespace X86ISA
-{
-    class GpuTLB : public ClockedObject
-    {
-      protected:
-        friend class Walker;
-
-        typedef std::list<TlbEntry*> EntryList;
-
-        uint32_t configAddress;
-
-      public:
-        typedef X86GPUTLBParams Params;
-        GpuTLB(const Params &p);
-        ~GpuTLB();
-
-        typedef enum BaseMMU::Mode Mode;
-
-        class Translation
-        {
-          public:
-            virtual ~Translation() { }
-
-            /**
-             * Signal that the translation has been delayed due to a hw page
-             * table walk.
-             */
-            virtual void markDelayed() = 0;
-
-            /**
-             * The memory for this object may be dynamically allocated, and it
-             * may be responsible for cleaning itslef up which will happen in
-             * this function. Once it's called the object is no longer valid.
-             */
-            virtual void finish(Fault fault, const RequestPtr &req,
-                                ThreadContext *tc, Mode mode) = 0;
-        };
-
-        void dumpAll();
-        TlbEntry *lookup(Addr va, bool update_lru=true);
-        void setConfigAddress(uint32_t addr);
-
-      protected:
-        EntryList::iterator lookupIt(Addr va, bool update_lru=true);
-        Walker *walker;
-
-      public:
-        Walker *getWalker();
-        void invalidateAll();
-        void invalidateNonGlobal();
-        void demapPage(Addr va, uint64_t asn);
-
-      protected:
-        int size;
-        int assoc;
-        int numSets;
-
-        /**
-         *  true if this is a fully-associative TLB
-         */
-        bool FA;
-        Addr setMask;
-
-        /**
-         * Allocation Policy: true if we always allocate on a hit, false
-         * otherwise. Default is true.
-         */
-        bool allocationPolicy;
-
-        /**
-         * if true, then this is not the last level TLB
-         */
-        bool hasMemSidePort;
-
-        /**
-         * Print out accessDistance stats. One stat file
-         * per TLB.
-         */
-        bool accessDistance;
-
-        std::vector<TlbEntry> tlb;
-
-        /*
-         * It's a per-set list. As long as we have not reached
-         * the full capacity of the given set, grab an entry from
-         * the freeList.
-         */
-        std::vector<EntryList> freeList;
-
-        /**
-         * An entryList per set is the equivalent of an LRU stack;
-         * it's used to guide replacement decisions. The head of the list
-         * contains the MRU TLB entry of the given set. If the freeList
-         * for this set is empty, the last element of the list
-         * is evicted (i.e., dropped on the floor).
-         */
-        std::vector<EntryList> entryList;
-
-        Fault translateInt(bool read, const RequestPtr &req,
-                           ThreadContext *tc);
-
-        Fault translate(const RequestPtr &req, ThreadContext *tc,
-                Translation *translation, Mode mode, bool &delayedResponse,
-                bool timing, int &latency);
-
-      public:
-        // latencies for a TLB hit, miss and page fault
-        int hitLatency;
-        int missLatency1;
-        int missLatency2;
-
-        void updatePageFootprint(Addr virt_page_addr);
-        void printAccessPattern();
-
-
-        Fault translateAtomic(const RequestPtr &req, ThreadContext *tc,
-                              Mode mode, int &latency);
-
-        void translateTiming(const RequestPtr &req, ThreadContext *tc,
-                             Translation *translation, Mode mode,
-                             int &latency);
-
-        Tick doMmuRegRead(ThreadContext *tc, Packet *pkt);
-        Tick doMmuRegWrite(ThreadContext *tc, Packet *pkt);
-
-        TlbEntry *insert(Addr vpn, TlbEntry &entry);
-
-        // Checkpointing
-        virtual void serialize(CheckpointOut& cp) const override;
-        virtual void unserialize(CheckpointIn& cp) override;
-        void issueTranslation();
-        enum tlbOutcome {TLB_HIT, TLB_MISS, PAGE_WALK, MISS_RETURN};
-        bool tlbLookup(const RequestPtr &req,
-                       ThreadContext *tc, bool update_stats);
-
-        void handleTranslationReturn(Addr addr, tlbOutcome outcome,
-                                     PacketPtr pkt);
-
-        void handleFuncTranslationReturn(PacketPtr pkt, tlbOutcome outcome);
-
-        void pagingProtectionChecks(ThreadContext *tc, PacketPtr pkt,
-                                    TlbEntry *tlb_entry, Mode mode);
-
-        void updatePhysAddresses(Addr virt_page_addr, TlbEntry *tlb_entry,
-                                 Addr phys_page_addr);
-
-        void issueTLBLookup(PacketPtr pkt);
-
-        // CpuSidePort is the TLB Port closer to the CPU/CU side
-        class CpuSidePort : public ResponsePort
-        {
-          public:
-            CpuSidePort(const std::string &_name, GpuTLB * gpu_TLB,
-                        PortID _index)
-                : ResponsePort(_name, gpu_TLB), tlb(gpu_TLB), index(_index) { }
-
-          protected:
-            GpuTLB *tlb;
-            int index;
-
-            virtual bool recvTimingReq(PacketPtr pkt);
-            virtual Tick recvAtomic(PacketPtr pkt) { return 0; }
-            virtual void recvFunctional(PacketPtr pkt);
-            virtual void recvRangeChange() { }
-            virtual void recvReqRetry();
-            virtual void recvRespRetry() { panic("recvRespRetry called"); }
-            virtual AddrRangeList getAddrRanges() const;
-        };
-
-        /**
-         * MemSidePort is the TLB Port closer to the memory side
-         * If this is a last level TLB then this port will not be connected.
-         *
-         * Future action item: if we ever do real page walks, then this port
-         * should be connected to a RubyPort.
-         */
-        class MemSidePort : public RequestPort
-        {
-          public:
-            MemSidePort(const std::string &_name, GpuTLB * gpu_TLB,
-                        PortID _index)
-                : RequestPort(_name, gpu_TLB), tlb(gpu_TLB), index(_index) { }
-
-            std::deque<PacketPtr> retries;
-
-          protected:
-            GpuTLB *tlb;
-            int index;
-
-            virtual bool recvTimingResp(PacketPtr pkt);
-            virtual Tick recvAtomic(PacketPtr pkt) { return 0; }
-            virtual void recvFunctional(PacketPtr pkt) { }
-            virtual void recvRangeChange() { }
-            virtual void recvReqRetry();
-        };
-
-        // TLB ports on the cpu Side
-        std::vector<CpuSidePort*> cpuSidePort;
-        // TLB ports on the memory side
-        std::vector<MemSidePort*> memSidePort;
-
-        Port &getPort(const std::string &if_name,
-                      PortID idx=InvalidPortID) override;
-
-        /**
-         * TLB TranslationState: this currently is a somewhat bastardization of
-         * the usage of SenderState, whereby the receiver of a packet is not
-         * usually supposed to need to look at the contents of the senderState,
-         * you're really only supposed to look at what you pushed on, pop it
-         * off, and send it back.
-         *
-         * However, since there is state that we want to pass to the TLBs using
-         * the send/recv Timing/Functional/etc. APIs, which don't allow for new
-         * arguments, we need a common TLB senderState to pass between TLBs,
-         * both "forwards" and "backwards."
-         *
-         * So, basically, the rule is that any packet received by a TLB port
-         * (cpuside OR memside) must be safely castable to a TranslationState.
-         */
-
-        struct TranslationState : public Packet::SenderState
-        {
-            // TLB mode, read or write
-            Mode tlbMode;
-            // Thread context associated with this req
-            ThreadContext *tc;
-
-            /*
-            * TLB entry to be populated and passed back and filled in
-            * previous TLBs.  Equivalent to the data cache concept of
-            * "data return."
-            */
-            TlbEntry *tlbEntry;
-            // Is this a TLB prefetch request?
-            bool isPrefetch;
-            // When was the req for this translation issued
-            uint64_t issueTime;
-            // Remember where this came from
-            std::vector<ResponsePort*>ports;
-
-            // keep track of #uncoalesced reqs per packet per TLB level;
-            // reqCnt per level >= reqCnt higher level
-            std::vector<int> reqCnt;
-            // TLB level this packet hit in; 0 if it hit in the page table
-            int hitLevel;
-            Packet::SenderState *saved;
-
-            TranslationState(Mode tlb_mode, ThreadContext *_tc,
-                             bool is_prefetch=false,
-                             Packet::SenderState *_saved=nullptr)
-                : tlbMode(tlb_mode), tc(_tc), tlbEntry(nullptr),
-                  isPrefetch(is_prefetch), issueTime(0),
-                  hitLevel(0),saved(_saved) { }
-        };
-
-        // maximum number of permitted coalesced requests per cycle
-        int maxCoalescedReqs;
-
-        // Current number of outstandings coalesced requests.
-        // Should be <= maxCoalescedReqs
-        int outstandingReqs;
-
-        /**
-         * A TLBEvent is scheduled after the TLB lookup and helps us take the
-         * appropriate actions:
-         *  (e.g., update TLB on a hit,
-         *  send request to lower level TLB on a miss,
-         *  or start a page walk if this was the last-level TLB).
-         */
-        void translationReturn(Addr virtPageAddr, tlbOutcome outcome,
-                               PacketPtr pkt);
-
-        class TLBEvent : public Event
-        {
-            private:
-                GpuTLB *tlb;
-                Addr virtPageAddr;
-                /**
-                 * outcome can be TLB_HIT, TLB_MISS, or PAGE_WALK
-                 */
-                tlbOutcome outcome;
-                PacketPtr pkt;
-
-            public:
-                TLBEvent(GpuTLB *_tlb, Addr _addr, tlbOutcome outcome,
-                        PacketPtr _pkt);
-
-                void process();
-                const char *description() const;
-
-                // updateOutcome updates the tlbOutcome of a TLBEvent
-                void updateOutcome(tlbOutcome _outcome);
-                Addr getTLBEventVaddr();
-        };
-
-        std::unordered_map<Addr, TLBEvent*> translationReturnEvent;
-
-        // this FIFO queue keeps track of the virt. page addresses
-        // that are pending cleanup
-        std::queue<Addr> cleanupQueue;
-
-        // the cleanupEvent is scheduled after a TLBEvent triggers in order to
-        // free memory and do the required clean-up
-        void cleanup();
-
-        EventFunctionWrapper cleanupEvent;
-
-        /**
-         * This hash map will use the virtual page address as a key
-         * and will keep track of total number of accesses per page
-         */
-
-        struct AccessInfo
-        {
-            unsigned int lastTimeAccessed; // last access to this page
-            unsigned int accessesPerPage;
-            // need to divide it by accessesPerPage at the end
-            unsigned int totalReuseDistance;
-
-            /**
-             * The field below will help us compute the access distance,
-             * that is the number of (coalesced) TLB accesses that
-             * happened in between each access to this page
-             *
-             * localTLBAccesses[x] is the value of localTLBNumAccesses
-             * when the page <Addr> was accessed for the <x>th time
-             */
-            std::vector<unsigned int> localTLBAccesses;
-            unsigned int sumDistance;
-            unsigned int meanDistance;
-        };
-
-        typedef std::unordered_map<Addr, AccessInfo> AccessPatternTable;
-        AccessPatternTable TLBFootprint;
-
-        // Called at the end of simulation to dump page access stats.
-        void exitCallback();
-
-        EventFunctionWrapper exitEvent;
-
-      protected:
-        struct GpuTLBStats : public statistics::Group
-        {
-            GpuTLBStats(statistics::Group *parent);
-
-            // local_stats are as seen from the TLB
-            // without taking into account coalescing
-            statistics::Scalar localNumTLBAccesses;
-            statistics::Scalar localNumTLBHits;
-            statistics::Scalar localNumTLBMisses;
-            statistics::Formula localTLBMissRate;
-
-            // global_stats are as seen from the
-            // CU's perspective taking into account
-            // all coalesced requests.
-            statistics::Scalar globalNumTLBAccesses;
-            statistics::Scalar globalNumTLBHits;
-            statistics::Scalar globalNumTLBMisses;
-            statistics::Formula globalTLBMissRate;
-
-            // from the CU perspective (global)
-            statistics::Scalar accessCycles;
-            // from the CU perspective (global)
-            statistics::Scalar pageTableCycles;
-            statistics::Scalar numUniquePages;
-            // from the perspective of this TLB
-            statistics::Scalar localCycles;
-            // from the perspective of this TLB
-            statistics::Formula localLatency;
-            // I take the avg. per page and then
-            // the avg. over all pages.
-            statistics::Scalar avgReuseDistance;
-        } stats;
-    };
-}
-
-using GpuTranslationState = X86ISA::GpuTLB::TranslationState;
-
-} // namespace gem5
-
-#endif // __GPU_TLB_HH__
diff --git a/src/arch/amdgpu/gcn3/tlb_coalescer.cc b/src/arch/amdgpu/gcn3/tlb_coalescer.cc
deleted file mode 100644
index 9b53db8..0000000
--- a/src/arch/amdgpu/gcn3/tlb_coalescer.cc
+++ /dev/null
@@ -1,539 +0,0 @@
-/*
- * Copyright (c) 2011-2015 Advanced Micro Devices, Inc.
- * All rights reserved.
- *
- * For use for simulation and test purposes only
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright notice,
- * this list of conditions and the following disclaimer.
- *
- * 2. Redistributions in binary form must reproduce the above copyright notice,
- * this list of conditions and the following disclaimer in the documentation
- * and/or other materials provided with the distribution.
- *
- * 3. Neither the name of the copyright holder nor the names of its
- * contributors may be used to endorse or promote products derived from this
- * software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "arch/amdgpu/gcn3/tlb_coalescer.hh"
-
-#include <cstring>
-
-#include "arch/x86/page_size.hh"
-#include "base/logging.hh"
-#include "debug/GPUTLB.hh"
-#include "sim/process.hh"
-
-namespace gem5
-{
-
-TLBCoalescer::TLBCoalescer(const Params &p)
-    : ClockedObject(p),
-      TLBProbesPerCycle(p.probesPerCycle),
-      coalescingWindow(p.coalescingWindow),
-      disableCoalescing(p.disableCoalescing),
-      probeTLBEvent([this]{ processProbeTLBEvent(); },
-                    "Probe the TLB below",
-                    false, Event::CPU_Tick_Pri),
-      cleanupEvent([this]{ processCleanupEvent(); },
-                   "Cleanup issuedTranslationsTable hashmap",
-                   false, Event::Maximum_Pri),
-      stats(this)
-{
-    // create the response ports based on the number of connected ports
-    for (size_t i = 0; i < p.port_cpu_side_ports_connection_count; ++i) {
-        cpuSidePort.push_back(new CpuSidePort(csprintf("%s-port%d", name(), i),
-                                              this, i));
-    }
-
-    // create the request ports based on the number of connected ports
-    for (size_t i = 0; i < p.port_mem_side_ports_connection_count; ++i) {
-        memSidePort.push_back(new MemSidePort(csprintf("%s-port%d", name(), i),
-                                              this, i));
-    }
-}
-
-Port &
-TLBCoalescer::getPort(const std::string &if_name, PortID idx)
-{
-    if (if_name == "cpu_side_ports") {
-        if (idx >= static_cast<PortID>(cpuSidePort.size())) {
-            panic("TLBCoalescer::getPort: unknown index %d\n", idx);
-        }
-
-        return *cpuSidePort[idx];
-    } else  if (if_name == "mem_side_ports") {
-        if (idx >= static_cast<PortID>(memSidePort.size())) {
-            panic("TLBCoalescer::getPort: unknown index %d\n", idx);
-        }
-
-        return *memSidePort[idx];
-    } else {
-        panic("TLBCoalescer::getPort: unknown port %s\n", if_name);
-    }
-}
-
-/*
- * This method returns true if the <incoming_pkt>
- * can be coalesced with <coalesced_pkt> and false otherwise.
- * A given set of rules is checked.
- * The rules can potentially be modified based on the TLB level.
- */
-bool
-TLBCoalescer::canCoalesce(PacketPtr incoming_pkt, PacketPtr coalesced_pkt)
-{
-    if (disableCoalescing)
-        return false;
-
-    GpuTranslationState *incoming_state =
-      safe_cast<GpuTranslationState*>(incoming_pkt->senderState);
-
-    GpuTranslationState *coalesced_state =
-     safe_cast<GpuTranslationState*>(coalesced_pkt->senderState);
-
-    // Rule 1: Coalesce requests only if they
-    // fall within the same virtual page
-    Addr incoming_virt_page_addr = roundDown(incoming_pkt->req->getVaddr(),
-                                             X86ISA::PageBytes);
-
-    Addr coalesced_virt_page_addr = roundDown(coalesced_pkt->req->getVaddr(),
-                                              X86ISA::PageBytes);
-
-    if (incoming_virt_page_addr != coalesced_virt_page_addr)
-        return false;
-
-    //* Rule 2: Coalesce requests only if they
-    // share a TLB Mode, i.e. they are both read
-    // or write requests.
-    BaseMMU::Mode incoming_mode = incoming_state->tlbMode;
-    BaseMMU::Mode coalesced_mode = coalesced_state->tlbMode;
-
-    if (incoming_mode != coalesced_mode)
-        return false;
-
-    // when we can coalesce a packet update the reqCnt
-    // that is the number of packets represented by
-    // this coalesced packet
-    if (!incoming_state->isPrefetch)
-        coalesced_state->reqCnt.back() += incoming_state->reqCnt.back();
-
-    return true;
-}
-
-/*
- * We need to update the physical addresses of all the translation requests
- * that were coalesced into the one that just returned.
- */
-void
-TLBCoalescer::updatePhysAddresses(PacketPtr pkt)
-{
-    Addr virt_page_addr = roundDown(pkt->req->getVaddr(), X86ISA::PageBytes);
-
-    DPRINTF(GPUTLB, "Update phys. addr. for %d coalesced reqs for page %#x\n",
-            issuedTranslationsTable[virt_page_addr].size(), virt_page_addr);
-
-    GpuTranslationState *sender_state =
-        safe_cast<GpuTranslationState*>(pkt->senderState);
-
-    TheISA::TlbEntry *tlb_entry = sender_state->tlbEntry;
-    assert(tlb_entry);
-    Addr first_entry_vaddr = tlb_entry->vaddr;
-    Addr first_entry_paddr = tlb_entry->paddr;
-    int page_size = tlb_entry->size();
-    bool uncacheable = tlb_entry->uncacheable;
-    int first_hit_level = sender_state->hitLevel;
-
-    // Get the physical page address of the translated request
-    // Using the page_size specified in the TLBEntry allows us
-    // to support different page sizes.
-    Addr phys_page_paddr = pkt->req->getPaddr();
-    phys_page_paddr &= ~(page_size - 1);
-
-    for (int i = 0; i < issuedTranslationsTable[virt_page_addr].size(); ++i) {
-        PacketPtr local_pkt = issuedTranslationsTable[virt_page_addr][i];
-        GpuTranslationState *sender_state =
-            safe_cast<GpuTranslationState*>(
-                    local_pkt->senderState);
-
-        // we are sending the packet back, so pop the reqCnt associated
-        // with this level in the TLB hiearchy
-        if (!sender_state->isPrefetch)
-            sender_state->reqCnt.pop_back();
-
-        /*
-         * Only the first packet from this coalesced request has been
-         * translated. Grab the translated phys. page addr and update the
-         * physical addresses of the remaining packets with the appropriate
-         * page offsets.
-         */
-        if (i) {
-            Addr paddr = phys_page_paddr;
-            paddr |= (local_pkt->req->getVaddr() & (page_size - 1));
-            local_pkt->req->setPaddr(paddr);
-
-            if (uncacheable)
-                local_pkt->req->setFlags(Request::UNCACHEABLE);
-
-            // update senderState->tlbEntry, so we can insert
-            // the correct TLBEentry in the TLBs above.
-            auto p = sender_state->tc->getProcessPtr();
-            sender_state->tlbEntry =
-                new TheISA::TlbEntry(p->pid(), first_entry_vaddr,
-                    first_entry_paddr, false, false);
-
-            // update the hitLevel for all uncoalesced reqs
-            // so that each packet knows where it hit
-            // (used for statistics in the CUs)
-            sender_state->hitLevel = first_hit_level;
-        }
-
-        ResponsePort *return_port = sender_state->ports.back();
-        sender_state->ports.pop_back();
-
-        // Translation is done - Convert to a response pkt if necessary and
-        // send the translation back
-        if (local_pkt->isRequest()) {
-            local_pkt->makeTimingResponse();
-        }
-
-        return_port->sendTimingResp(local_pkt);
-    }
-
-    // schedule clean up for end of this cycle
-    // This is a maximum priority event and must be on
-    // the same cycle as GPUTLB cleanup event to prevent
-    // race conditions with an IssueProbeEvent caused by
-    // MemSidePort::recvReqRetry
-    cleanupQueue.push(virt_page_addr);
-
-    if (!cleanupEvent.scheduled())
-        schedule(cleanupEvent, curTick());
-}
-
-// Receive translation requests, create a coalesced request,
-// and send them to the TLB (TLBProbesPerCycle)
-bool
-TLBCoalescer::CpuSidePort::recvTimingReq(PacketPtr pkt)
-{
-    // first packet of a coalesced request
-    PacketPtr first_packet = nullptr;
-    // true if we are able to do coalescing
-    bool didCoalesce = false;
-    // number of coalesced reqs for a given window
-    int coalescedReq_cnt = 0;
-
-    GpuTranslationState *sender_state =
-        safe_cast<GpuTranslationState*>(pkt->senderState);
-
-    // push back the port to remember the path back
-    sender_state->ports.push_back(this);
-
-    bool update_stats = !sender_state->isPrefetch;
-
-    if (update_stats) {
-        // if reqCnt is empty then this packet does not represent
-        // multiple uncoalesced reqs(pkts) but just a single pkt.
-        // If it does though then the reqCnt for each level in the
-        // hierarchy accumulates the total number of reqs this packet
-        // represents
-        int req_cnt = 1;
-
-        if (!sender_state->reqCnt.empty())
-            req_cnt = sender_state->reqCnt.back();
-
-        sender_state->reqCnt.push_back(req_cnt);
-
-        // update statistics
-        coalescer->stats.uncoalescedAccesses++;
-        req_cnt = sender_state->reqCnt.back();
-        DPRINTF(GPUTLB, "receiving pkt w/ req_cnt %d\n", req_cnt);
-        coalescer->stats.queuingCycles -= (curTick() * req_cnt);
-        coalescer->stats.localqueuingCycles -= curTick();
-    }
-
-    // FIXME if you want to coalesce not based on the issueTime
-    // of the packets (i.e., from the compute unit's perspective)
-    // but based on when they reached this coalescer then
-    // remove the following if statement and use curTick() or
-    // coalescingWindow for the tick_index.
-    if (!sender_state->issueTime)
-       sender_state->issueTime = curTick();
-
-    // The tick index is used as a key to the coalescerFIFO hashmap.
-    // It is shared by all candidates that fall within the
-    // given coalescingWindow.
-    int64_t tick_index = sender_state->issueTime / coalescer->coalescingWindow;
-
-    if (coalescer->coalescerFIFO.count(tick_index)) {
-        coalescedReq_cnt = coalescer->coalescerFIFO[tick_index].size();
-    }
-
-    // see if we can coalesce the incoming pkt with another
-    // coalesced request with the same tick_index
-    for (int i = 0; i < coalescedReq_cnt; ++i) {
-        first_packet = coalescer->coalescerFIFO[tick_index][i][0];
-
-        if (coalescer->canCoalesce(pkt, first_packet)) {
-            coalescer->coalescerFIFO[tick_index][i].push_back(pkt);
-
-            DPRINTF(GPUTLB, "Coalesced req %i w/ tick_index %d has %d reqs\n",
-                    i, tick_index,
-                    coalescer->coalescerFIFO[tick_index][i].size());
-
-            didCoalesce = true;
-            break;
-        }
-    }
-
-    // if this is the first request for this tick_index
-    // or we did not manage to coalesce, update stats
-    // and make necessary allocations.
-    if (!coalescedReq_cnt || !didCoalesce) {
-        if (update_stats)
-            coalescer->stats.coalescedAccesses++;
-
-        std::vector<PacketPtr> new_array;
-        new_array.push_back(pkt);
-        coalescer->coalescerFIFO[tick_index].push_back(new_array);
-
-        DPRINTF(GPUTLB, "coalescerFIFO[%d] now has %d coalesced reqs after "
-                "push\n", tick_index,
-                coalescer->coalescerFIFO[tick_index].size());
-    }
-
-    //schedule probeTLBEvent next cycle to send the
-    //coalesced requests to the TLB
-    if (!coalescer->probeTLBEvent.scheduled()) {
-        coalescer->schedule(coalescer->probeTLBEvent,
-                curTick() + coalescer->clockPeriod());
-    }
-
-    return true;
-}
-
-void
-TLBCoalescer::CpuSidePort::recvReqRetry()
-{
-    panic("recvReqRetry called");
-}
-
-void
-TLBCoalescer::CpuSidePort::recvFunctional(PacketPtr pkt)
-{
-
-    GpuTranslationState *sender_state =
-        safe_cast<GpuTranslationState*>(pkt->senderState);
-
-    bool update_stats = !sender_state->isPrefetch;
-
-    if (update_stats)
-        coalescer->stats.uncoalescedAccesses++;
-
-    // If there is a pending timing request for this virtual address
-    // print a warning message. This is a temporary caveat of
-    // the current simulator where atomic and timing requests can
-    // coexist. FIXME remove this check/warning in the future.
-    Addr virt_page_addr = roundDown(pkt->req->getVaddr(), X86ISA::PageBytes);
-    int map_count = coalescer->issuedTranslationsTable.count(virt_page_addr);
-
-    if (map_count) {
-        DPRINTF(GPUTLB, "Warning! Functional access to addr %#x sees timing "
-                "req. pending\n", virt_page_addr);
-    }
-
-    coalescer->memSidePort[0]->sendFunctional(pkt);
-}
-
-AddrRangeList
-TLBCoalescer::CpuSidePort::getAddrRanges() const
-{
-    // currently not checked by the requestor
-    AddrRangeList ranges;
-
-    return ranges;
-}
-
-bool
-TLBCoalescer::MemSidePort::recvTimingResp(PacketPtr pkt)
-{
-    // a translation completed and returned
-    coalescer->updatePhysAddresses(pkt);
-
-    return true;
-}
-
-void
-TLBCoalescer::MemSidePort::recvReqRetry()
-{
-    //we've receeived a retry. Schedule a probeTLBEvent
-    if (!coalescer->probeTLBEvent.scheduled())
-        coalescer->schedule(coalescer->probeTLBEvent,
-                curTick() + coalescer->clockPeriod());
-}
-
-void
-TLBCoalescer::MemSidePort::recvFunctional(PacketPtr pkt)
-{
-    fatal("Memory side recvFunctional() not implemented in TLB coalescer.\n");
-}
-
-/*
- * Here we scan the coalescer FIFO and issue the max
- * number of permitted probes to the TLB below. We
- * permit bypassing of coalesced requests for the same
- * tick_index.
- *
- * We do not access the next tick_index unless we've
- * drained the previous one. The coalesced requests
- * that are successfully sent are moved to the
- * issuedTranslationsTable table (the table which keeps
- * track of the outstanding reqs)
- */
-void
-TLBCoalescer::processProbeTLBEvent()
-{
-    // number of TLB probes sent so far
-    int sent_probes = 0;
-    // rejected denotes a blocking event
-    bool rejected = false;
-
-    // It is set to true either when the recvTiming of the TLB below
-    // returns false or when there is another outstanding request for the
-    // same virt. page.
-
-    DPRINTF(GPUTLB, "triggered TLBCoalescer %s\n", __func__);
-
-    for (auto iter = coalescerFIFO.begin();
-         iter != coalescerFIFO.end() && !rejected; ) {
-        int coalescedReq_cnt = iter->second.size();
-        int i = 0;
-        int vector_index = 0;
-
-        DPRINTF(GPUTLB, "coalescedReq_cnt is %d for tick_index %d\n",
-               coalescedReq_cnt, iter->first);
-
-        while (i < coalescedReq_cnt) {
-            ++i;
-            PacketPtr first_packet = iter->second[vector_index][0];
-
-            // compute virtual page address for this request
-            Addr virt_page_addr = roundDown(first_packet->req->getVaddr(),
-                    X86ISA::PageBytes);
-
-            // is there another outstanding request for the same page addr?
-            int pending_reqs =
-                issuedTranslationsTable.count(virt_page_addr);
-
-            if (pending_reqs) {
-                DPRINTF(GPUTLB, "Cannot issue - There are pending reqs for "
-                        "page %#x\n", virt_page_addr);
-
-                ++vector_index;
-                rejected = true;
-
-                continue;
-            }
-
-            // send the coalesced request for virt_page_addr
-            if (!memSidePort[0]->sendTimingReq(first_packet)) {
-                DPRINTF(GPUTLB, "Failed to send TLB request for page %#x\n",
-                       virt_page_addr);
-
-                // No need for a retries queue since we are already buffering
-                // the coalesced request in coalescerFIFO.
-                rejected = true;
-                ++vector_index;
-            } else {
-                GpuTranslationState *tmp_sender_state =
-                    safe_cast<GpuTranslationState*>
-                    (first_packet->senderState);
-
-                bool update_stats = !tmp_sender_state->isPrefetch;
-
-                if (update_stats) {
-                    // req_cnt is total number of packets represented
-                    // by the one we just sent counting all the way from
-                    // the top of TLB hiearchy (i.e., from the CU)
-                    int req_cnt = tmp_sender_state->reqCnt.back();
-                    stats.queuingCycles += (curTick() * req_cnt);
-
-                    DPRINTF(GPUTLB, "%s sending pkt w/ req_cnt %d\n",
-                            name(), req_cnt);
-
-                    // pkt_cnt is number of packets we coalesced into the one
-                    // we just sent but only at this coalescer level
-                    int pkt_cnt = iter->second[vector_index].size();
-                    stats.localqueuingCycles += (curTick() * pkt_cnt);
-                }
-
-                DPRINTF(GPUTLB, "Successfully sent TLB request for page %#x",
-                       virt_page_addr);
-
-                //copy coalescedReq to issuedTranslationsTable
-                issuedTranslationsTable[virt_page_addr]
-                    = iter->second[vector_index];
-
-                //erase the entry of this coalesced req
-                iter->second.erase(iter->second.begin() + vector_index);
-
-                if (iter->second.empty())
-                    assert(i == coalescedReq_cnt);
-
-                sent_probes++;
-                if (sent_probes == TLBProbesPerCycle)
-                   return;
-            }
-        }
-
-        //if there are no more coalesced reqs for this tick_index
-        //erase the hash_map with the first iterator
-        if (iter->second.empty()) {
-            coalescerFIFO.erase(iter++);
-        } else {
-            ++iter;
-        }
-    }
-}
-
-void
-TLBCoalescer::processCleanupEvent()
-{
-    while (!cleanupQueue.empty()) {
-        Addr cleanup_addr = cleanupQueue.front();
-        cleanupQueue.pop();
-        issuedTranslationsTable.erase(cleanup_addr);
-
-        DPRINTF(GPUTLB, "Cleanup - Delete coalescer entry with key %#x\n",
-                cleanup_addr);
-    }
-}
-
-TLBCoalescer::TLBCoalescerStats::TLBCoalescerStats(statistics::Group *parent)
-    : statistics::Group(parent),
-      ADD_STAT(uncoalescedAccesses, "Number of uncoalesced TLB accesses"),
-      ADD_STAT(coalescedAccesses, "Number of coalesced TLB accesses"),
-      ADD_STAT(queuingCycles, "Number of cycles spent in queue"),
-      ADD_STAT(localqueuingCycles,
-               "Number of cycles spent in queue for all incoming reqs"),
-      ADD_STAT(localLatency, "Avg. latency over all incoming pkts")
-{
-    localLatency = localqueuingCycles / uncoalescedAccesses;
-}
-
-} // namespace gem5
diff --git a/src/arch/amdgpu/gcn3/tlb_coalescer.hh b/src/arch/amdgpu/gcn3/tlb_coalescer.hh
deleted file mode 100644
index afe12c9..0000000
--- a/src/arch/amdgpu/gcn3/tlb_coalescer.hh
+++ /dev/null
@@ -1,226 +0,0 @@
-/*
- * Copyright (c) 2011-2015 Advanced Micro Devices, Inc.
- * All rights reserved.
- *
- * For use for simulation and test purposes only
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright notice,
- * this list of conditions and the following disclaimer.
- *
- * 2. Redistributions in binary form must reproduce the above copyright notice,
- * this list of conditions and the following disclaimer in the documentation
- * and/or other materials provided with the distribution.
- *
- * 3. Neither the name of the copyright holder nor the names of its
- * contributors may be used to endorse or promote products derived from this
- * software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
- * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- * POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef __TLB_COALESCER_HH__
-#define __TLB_COALESCER_HH__
-
-#include <list>
-#include <queue>
-#include <string>
-#include <vector>
-
-#include "arch/amdgpu/gcn3/tlb.hh"
-#include "arch/generic/tlb.hh"
-#include "arch/x86/isa.hh"
-#include "arch/x86/pagetable.hh"
-#include "arch/x86/regs/segment.hh"
-#include "base/logging.hh"
-#include "base/statistics.hh"
-#include "mem/port.hh"
-#include "mem/request.hh"
-#include "params/TLBCoalescer.hh"
-#include "sim/clocked_object.hh"
-
-namespace gem5
-{
-
-class BaseTLB;
-class Packet;
-class ThreadContext;
-
-/**
- * The TLBCoalescer is a ClockedObject sitting on the front side (CPUSide) of
- * each TLB. It receives packets and issues coalesced requests to the
- * TLB below it. It controls how requests are coalesced (the rules)
- * and the permitted number of TLB probes per cycle (i.e., how many
- * coalesced requests it feeds the TLB per cycle).
- */
-class TLBCoalescer : public ClockedObject
-{
-  public:
-    typedef TLBCoalescerParams Params;
-    TLBCoalescer(const Params &p);
-    ~TLBCoalescer() { }
-
-    // Number of TLB probes per cycle. Parameterizable - default 2.
-    int TLBProbesPerCycle;
-
-    // Consider coalescing across that many ticks.
-    // Paraemterizable - default 1.
-    int coalescingWindow;
-
-    // Each coalesced request consists of multiple packets
-    // that all fall within the same virtual page
-    typedef std::vector<PacketPtr> coalescedReq;
-
-    // disables coalescing when true
-    bool disableCoalescing;
-
-    /*
-     * This is a hash map with <tick_index> as a key.
-     * It contains a vector of coalescedReqs per <tick_index>.
-     * Requests are buffered here until they can be issued to
-     * the TLB, at which point they are copied to the
-     * issuedTranslationsTable hash map.
-     *
-     * In terms of coalescing, we coalesce requests in a given
-     * window of x cycles by using tick_index = issueTime/x as a
-     * key, where x = coalescingWindow. issueTime is the issueTime
-     * of the pkt from the ComputeUnit's perspective, but another
-     * option is to change it to curTick(), so we coalesce based
-     * on the receive time.
-     */
-    typedef std::map<int64_t, std::vector<coalescedReq>>
-        CoalescingFIFO;
-
-    CoalescingFIFO coalescerFIFO;
-
-    /*
-     * issuedTranslationsTabler: a hash_map indexed by virtual page
-     * address. Each hash_map entry has a vector of PacketPtr associated
-     * with it denoting the different packets that share an outstanding
-     * coalesced translation request for the same virtual page.
-     *
-     * The rules that determine which requests we can coalesce are
-     * specified in the canCoalesce() method.
-     */
-    typedef std::unordered_map<Addr, coalescedReq> CoalescingTable;
-
-    CoalescingTable issuedTranslationsTable;
-
-    bool canCoalesce(PacketPtr pkt1, PacketPtr pkt2);
-    void updatePhysAddresses(PacketPtr pkt);
-
-    class CpuSidePort : public ResponsePort
-    {
-      public:
-        CpuSidePort(const std::string &_name, TLBCoalescer *tlb_coalescer,
-                    PortID _index)
-            : ResponsePort(_name, tlb_coalescer), coalescer(tlb_coalescer),
-              index(_index) { }
-
-      protected:
-        TLBCoalescer *coalescer;
-        int index;
-
-        virtual bool recvTimingReq(PacketPtr pkt);
-        virtual Tick recvAtomic(PacketPtr pkt) { return 0; }
-        virtual void recvFunctional(PacketPtr pkt);
-        virtual void recvRangeChange() { }
-        virtual void recvReqRetry();
-
-        virtual void
-        recvRespRetry()
-        {
-            fatal("recvRespRetry() is not implemented in the TLB "
-                "coalescer.\n");
-        }
-
-        virtual AddrRangeList getAddrRanges() const;
-    };
-
-    class MemSidePort : public RequestPort
-    {
-      public:
-        MemSidePort(const std::string &_name, TLBCoalescer *tlb_coalescer,
-                    PortID _index)
-            : RequestPort(_name, tlb_coalescer), coalescer(tlb_coalescer),
-              index(_index) { }
-
-        std::deque<PacketPtr> retries;
-
-      protected:
-        TLBCoalescer *coalescer;
-        int index;
-
-        virtual bool recvTimingResp(PacketPtr pkt);
-        virtual Tick recvAtomic(PacketPtr pkt) { return 0; }
-        virtual void recvFunctional(PacketPtr pkt);
-        virtual void recvRangeChange() { }
-        virtual void recvReqRetry();
-
-        virtual void
-        recvRespRetry()
-        {
-            fatal("recvRespRetry() not implemented in TLB coalescer");
-        }
-    };
-
-    // Coalescer response ports on the cpu Side
-    std::vector<CpuSidePort*> cpuSidePort;
-    // Coalescer request ports on the memory side
-    std::vector<MemSidePort*> memSidePort;
-
-    Port &getPort(const std::string &if_name,
-                  PortID idx=InvalidPortID) override;
-
-    void processProbeTLBEvent();
-    /// This event issues the TLB probes
-    EventFunctionWrapper probeTLBEvent;
-
-    void processCleanupEvent();
-    /// The cleanupEvent is scheduled after a TLBEvent triggers
-    /// in order to free memory and do the required clean-up
-    EventFunctionWrapper cleanupEvent;
-
-    // this FIFO queue keeps track of the virt. page
-    // addresses that are pending cleanup
-    std::queue<Addr> cleanupQueue;
-
-  protected:
-    struct TLBCoalescerStats : public statistics::Group
-    {
-        TLBCoalescerStats(statistics::Group *parent);
-
-        // number of packets the coalescer receives
-        statistics::Scalar uncoalescedAccesses;
-        // number packets the coalescer send to the TLB
-        statistics::Scalar coalescedAccesses;
-
-        // Number of cycles the coalesced requests spend waiting in
-        // coalescerFIFO. For each packet the coalescer receives we take into
-        // account the number of all uncoalesced requests this pkt "represents"
-        statistics::Scalar queuingCycles;
-
-        // On average how much time a request from the
-        // uncoalescedAccesses that reaches the TLB
-        // spends waiting?
-        statistics::Scalar localqueuingCycles;
-        // localqueuingCycles/uncoalescedAccesses
-        statistics::Formula localLatency;
-    } stats;
-};
-
-} // namespace gem5
-
-#endif // __TLB_COALESCER_HH__
diff --git a/src/gpu-compute/compute_unit.cc b/src/gpu-compute/compute_unit.cc
index feef552..761d37b 100644
--- a/src/gpu-compute/compute_unit.cc
+++ b/src/gpu-compute/compute_unit.cc
@@ -35,6 +35,7 @@
 
 #include <limits>
 
+#include "arch/amdgpu/common/tlb.hh"
 #include "base/output.hh"
 #include "debug/GPUDisp.hh"
 #include "debug/GPUExec.hh"
diff --git a/src/gpu-compute/fetch_unit.cc b/src/gpu-compute/fetch_unit.cc
index 437a48d..fed9cb5 100644
--- a/src/gpu-compute/fetch_unit.cc
+++ b/src/gpu-compute/fetch_unit.cc
@@ -33,6 +33,7 @@
 
 #include "gpu-compute/fetch_unit.hh"
 
+#include "arch/amdgpu/common/tlb.hh"
 #include "base/bitfield.hh"
 #include "debug/GPUFetch.hh"
 #include "debug/GPUPort.hh"
diff --git a/src/gpu-compute/shader.cc b/src/gpu-compute/shader.cc
index ad18d01..4d71ca0 100644
--- a/src/gpu-compute/shader.cc
+++ b/src/gpu-compute/shader.cc
@@ -35,6 +35,7 @@
 
 #include <limits>
 
+#include "arch/amdgpu/common/tlb.hh"
 #include "base/chunk_generator.hh"
 #include "debug/GPUAgentDisp.hh"
 #include "debug/GPUDisp.hh"
-- 
1.8.3.1

